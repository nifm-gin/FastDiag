{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5cae00-a46a-433b-b0a7-7110593f5663",
   "metadata": {},
   "source": [
    "### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb4cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.warn = warn\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore::UserWarning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0336203-bd6a-4327-9b9a-477037a13a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from utility.utility import false_neg_scorer, false_pos_scorer\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "from get_outcomes import get_mortality_6m\n",
    "import warnings\n",
    "#import shap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14779e12-7eae-4b43-9729-b973d9f9c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba999991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name  mortality\n",
      "0  P0000          0\n",
      "1  P0001          0\n",
      "2  P0002          0\n",
      "3  P0003          0\n",
      "4  P0004          0\n",
      "Outcome events : 33\n"
     ]
    }
   ],
   "source": [
    "y = get_mortality_6m()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0ff40-f90c-4b48-ab18-3816a9d6785d",
   "metadata": {},
   "source": [
    "### Preprocessing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e7956ed-1c05-45e4-852e-cd49b1d584ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>supratentorial_IPH</th>\n",
       "      <th>supratentorial_SAH</th>\n",
       "      <th>supratentorial_Petechiae</th>\n",
       "      <th>supratentorial_Edema</th>\n",
       "      <th>infratentorial_IPH</th>\n",
       "      <th>infratentorial_SAH</th>\n",
       "      <th>infratentorial_Petechiae</th>\n",
       "      <th>infratentorial_Edema</th>\n",
       "      <th>brainstem_IPH</th>\n",
       "      <th>...</th>\n",
       "      <th>pression_arterielle_diastolique_PAD_arrivee_du_smur</th>\n",
       "      <th>score_glasgow_initial</th>\n",
       "      <th>score_glasgow_moteur_initial</th>\n",
       "      <th>anomalie_pupillaire_prehospitalier</th>\n",
       "      <th>frequence_cardiaque_FC_arrivee_du_smur</th>\n",
       "      <th>arret_cardio_respiratoire_massage</th>\n",
       "      <th>penetrant_objet</th>\n",
       "      <th>ischemie_du_membre</th>\n",
       "      <th>hemorragie_externe</th>\n",
       "      <th>amputation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0001</td>\n",
       "      <td>0</td>\n",
       "      <td>342</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0003</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0004</td>\n",
       "      <td>0</td>\n",
       "      <td>328</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0005</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  supratentorial_IPH  supratentorial_SAH  supratentorial_Petechiae  \\\n",
       "0  P0001                   0                 342                         0   \n",
       "1  P0002                   0                   0                         0   \n",
       "2  P0003                   0                 101                         0   \n",
       "3  P0004                   0                 328                         0   \n",
       "4  P0005                   0                   9                         0   \n",
       "\n",
       "   supratentorial_Edema  infratentorial_IPH  infratentorial_SAH  \\\n",
       "0                     0                   0                  15   \n",
       "1                     0                   0                   0   \n",
       "2                     0                   0                   0   \n",
       "3                     0                   0                   0   \n",
       "4                    15                   0                   0   \n",
       "\n",
       "   infratentorial_Petechiae  infratentorial_Edema  brainstem_IPH  ...  \\\n",
       "0                         0                     0              0  ...   \n",
       "1                         0                     0              0  ...   \n",
       "2                         0                     0              0  ...   \n",
       "3                         0                     0              0  ...   \n",
       "4                         0                     0              0  ...   \n",
       "\n",
       "   pression_arterielle_diastolique_PAD_arrivee_du_smur  score_glasgow_initial  \\\n",
       "0                                               49.0                     15.0   \n",
       "1                                               60.0                     15.0   \n",
       "2                                               64.0                     14.0   \n",
       "3                                               71.0                     15.0   \n",
       "4                                               79.0                      NaN   \n",
       "\n",
       "   score_glasgow_moteur_initial  anomalie_pupillaire_prehospitalier  \\\n",
       "0                           6.0                                 0.0   \n",
       "1                           6.0                                 0.0   \n",
       "2                           6.0                                 0.0   \n",
       "3                           6.0                                 0.0   \n",
       "4                           NaN                                 0.0   \n",
       "\n",
       "   frequence_cardiaque_FC_arrivee_du_smur  arret_cardio_respiratoire_massage  \\\n",
       "0                                    56.0                                0.0   \n",
       "1                                   100.0                                0.0   \n",
       "2                                   120.0                                0.0   \n",
       "3                                   107.0                                0.0   \n",
       "4                                    83.0                                0.0   \n",
       "\n",
       "   penetrant_objet  ischemie_du_membre  hemorragie_externe  amputation  \n",
       "0              0.0                 0.0                 0.0         0.0  \n",
       "1              0.0                 0.0                 0.0         0.0  \n",
       "2              0.0                 0.0                 0.0         0.0  \n",
       "3              0.0                 0.0                 0.0         0.0  \n",
       "4              0.0                 0.0                 0.0         0.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_volumes_clinical = pd.read_csv(DATA_DIRECTORY+\"segmentation_data_anonymized.csv\", usecols=range(1,31))\n",
    "X_volumes_clinical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24face50-fac0-4b63-bb9f-3b99fa422428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>hemocue_initial</th>\n",
       "      <th>fracas_du_bassin</th>\n",
       "      <th>catecholamines</th>\n",
       "      <th>pression_arterielle_systolique_PAS_arrivee_du_smur</th>\n",
       "      <th>pression_arterielle_diastolique_PAD_arrivee_du_smur</th>\n",
       "      <th>score_glasgow_initial</th>\n",
       "      <th>score_glasgow_moteur_initial</th>\n",
       "      <th>anomalie_pupillaire_prehospitalier</th>\n",
       "      <th>frequence_cardiaque_FC_arrivee_du_smur</th>\n",
       "      <th>arret_cardio_respiratoire_massage</th>\n",
       "      <th>penetrant_objet</th>\n",
       "      <th>ischemie_du_membre</th>\n",
       "      <th>hemorragie_externe</th>\n",
       "      <th>amputation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0001</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0002</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0003</td>\n",
       "      <td>42.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0004</td>\n",
       "      <td>34.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0005</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name   age  hemocue_initial  fracas_du_bassin  catecholamines  \\\n",
       "0  P0001  52.0              NaN               0.0             0.0   \n",
       "1  P0002  23.0              NaN               0.0             0.0   \n",
       "2  P0003  42.0             13.1               0.0             0.0   \n",
       "3  P0004  34.0             15.8               0.0             0.0   \n",
       "4  P0005  22.0              NaN               0.0             0.0   \n",
       "\n",
       "   pression_arterielle_systolique_PAS_arrivee_du_smur  \\\n",
       "0                                               87.0    \n",
       "1                                              100.0    \n",
       "2                                              101.0    \n",
       "3                                              110.0    \n",
       "4                                              114.0    \n",
       "\n",
       "   pression_arterielle_diastolique_PAD_arrivee_du_smur  score_glasgow_initial  \\\n",
       "0                                               49.0                     15.0   \n",
       "1                                               60.0                     15.0   \n",
       "2                                               64.0                     14.0   \n",
       "3                                               71.0                     15.0   \n",
       "4                                               79.0                      NaN   \n",
       "\n",
       "   score_glasgow_moteur_initial  anomalie_pupillaire_prehospitalier  \\\n",
       "0                           6.0                                 0.0   \n",
       "1                           6.0                                 0.0   \n",
       "2                           6.0                                 0.0   \n",
       "3                           6.0                                 0.0   \n",
       "4                           NaN                                 0.0   \n",
       "\n",
       "   frequence_cardiaque_FC_arrivee_du_smur  arret_cardio_respiratoire_massage  \\\n",
       "0                                    56.0                                0.0   \n",
       "1                                   100.0                                0.0   \n",
       "2                                   120.0                                0.0   \n",
       "3                                   107.0                                0.0   \n",
       "4                                    83.0                                0.0   \n",
       "\n",
       "   penetrant_objet  ischemie_du_membre  hemorragie_externe  amputation  \n",
       "0              0.0                 0.0                 0.0         0.0  \n",
       "1              0.0                 0.0                 0.0         0.0  \n",
       "2              0.0                 0.0                 0.0         0.0  \n",
       "3              0.0                 0.0                 0.0         0.0  \n",
       "4              0.0                 0.0                 0.0         0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_load = list(range(16, 31)) + [1]\n",
    "\n",
    "X_clinical_only = pd.read_csv(DATA_DIRECTORY+\"segmentation_data_anonymized.csv\", usecols=columns_to_load)\n",
    "X_clinical_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829293f-4f10-4749-bd2d-6749e83c1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify indexes where 'mortality' is NaN in y\n",
    "nan_indexes = y.loc[pd.isna(y[\"mortality\"]), :].index\n",
    "print(f\"Indexes with NaN in 'mortality': {nan_indexes}\")\n",
    "\n",
    "# Drop rows with NaN in y\n",
    "y = y.drop(index=nan_indexes)\n",
    "\n",
    "# Ensure 'name' is the index or used for alignment\n",
    "X_volumes_clinical = X_volumes_clinical[~X_volumes_clinical['name'].isin(y.loc[nan_indexes, 'name'])]\n",
    "\n",
    "# Verify the shapes after cleaning\n",
    "print(f\"Shape of y after cleaning: {y.shape}\")\n",
    "print(f\"Shape of X_volumes_clinical after cleaning: {X_volumes_clinical.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c5d98-ede1-45dc-8aa2-e5addd948d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VÃ©rifiez les valeurs uniques de 'name' dans y\n",
    "print(\"Unique 'name' values in y:\", y['name'].nunique())\n",
    "\n",
    "# VÃ©rifiez les valeurs uniques de 'name' dans X_volumes_clinical_with_outcome\n",
    "print(\"Unique 'name' values in X_volumes_clinical:\", X_volumes_clinical['name'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1063a3-b21f-4c73-942d-ababf0c47770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noms dans y mais pas dans X_volumes_clinical\n",
    "missing_in_X = set(y['name']) - set(X_volumes_clinical['name'])\n",
    "print(f\"Names in y but not in X_volumes_clinical: {missing_in_X}\")\n",
    "\n",
    "# Noms dans X_volumes_clinical mais pas dans y\n",
    "missing_in_y = set(X_volumes_clinical['name']) - set(y['name'])\n",
    "print(f\"Names in X_volumes_clinical but not in y: {missing_in_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb1d58-c0a7-438f-82be-f64e1794df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver les noms communs\n",
    "common_names = set(y['name']).intersection(set(X_volumes_clinical['name']))\n",
    "\n",
    "# Filtrer y et X_volumes_clinical_with_outcome pour ne garder que les noms communs\n",
    "y = y[y['name'].isin(common_names)]\n",
    "X_volumes_clinical = X_volumes_clinical[X_volumes_clinical['name'].isin(common_names)]\n",
    "\n",
    "# VÃ©rifiez les dimensions aprÃ¨s nettoyage\n",
    "print(f\"Shape of y after alignment: {y.shape}\")\n",
    "print(f\"Shape of X_volumes_clinical_with_outcome after alignment: {X_volumes_clinical.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff8e56-856d-41da-9685-91de3a139fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in 'name' in y\n",
    "print(\"Number of duplicate names in y:\", y['name'].duplicated().sum())\n",
    "\n",
    "# Check for duplicates in 'name' in X_volumes_clinical_with_outcome\n",
    "print(\"Number of duplicate names in X_volumes_clinical:\", X_volumes_clinical['name'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162eebf-0d5d-4fdc-9d65-bca6a9205245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from both DataFrames\n",
    "y = y.drop_duplicates(subset=['name'])\n",
    "X_volumes_clinical = X_volumes_clinical.drop_duplicates(subset=['name'])\n",
    "\n",
    "# Verify the shapes again\n",
    "print(f\"Shape of y after removing duplicates: {y.shape}\")\n",
    "print(f\"Shape of X_volumes_clinical after removing duplicates: {X_volumes_clinical.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d4c78-3a64-4210-8544-5ef7dfb7643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_volumes_clinical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b1b63-86f3-4377-a00f-5030d037b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'name' values are the same in both DataFrames\n",
    "common_names = set(y['name']) == set(X_volumes_clinical['name'])\n",
    "print(f\"Are 'name' values aligned between y and X_volumes_clinical? {common_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8953658-c6f7-4d3d-8f4e-f031e01bd9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier y et X par la colonne 'name'\n",
    "y = y.sort_values(by='name').reset_index(drop=True)\n",
    "X_volumes_clinical = X_volumes_clinical.sort_values(by='name').reset_index(drop=True)\n",
    "\n",
    "# VÃ©rifier si les noms sont alignÃ©s\n",
    "print((y['name'].values == X_volumes_clinical['name'].values).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d1ca75-c88f-40ef-ace8-d0746678517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'name' column from X_volumes_clinical_with_outcome\n",
    "X_features = X_volumes_clinical.drop(columns=['name'])\n",
    "\n",
    "# imputation with median \n",
    "X_features_imputed = X_features.fillna(X_features.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6515ad1-0dc1-4382-8d60-bcd121e25902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'name' column from y (if it exists)\n",
    "y_outcome = y.drop(columns=['name'])\n",
    "\n",
    "# Ensure y_outcome is a 1D array\n",
    "y_outcome = y_outcome.values.ravel()  # Convert to 1D if using pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948613f6-461a-446a-bfa0-8f704738c35c",
   "metadata": {},
   "source": [
    "### Gradient boosting with traumatrix model + segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ddede",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4719e-1d7e-4c18-9604-d76a947b44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "N_REPEATS = 3\n",
    "nb_total_samples = len(y_outcome)\n",
    "\n",
    "pipeline_smote_under = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf = GridSearchCV(estimator=pipeline_smote_under, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv)\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling = cross_validate(clf, X_features_imputed, y_outcome, \n",
    "                                                   scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, \n",
    "                                                   cv=outer_cv, n_jobs=-1, return_estimator=True, return_indices=True)\n",
    "\n",
    "print(\"HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "for estimator in nested_scores_smote_undersampling['estimator']:\n",
    "    print(estimator.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e25fb62",
   "metadata": {},
   "source": [
    "#### Get individual test set samples prediction vs ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(nested_scores_smote_undersampling[\"indices\"][\"test\"])):\n",
    "    if 469 in nested_scores_smote_undersampling[\"indices\"][\"test\"][i]:\n",
    "        print(f'fold={i}')\n",
    "    #print(f\"fold={i}\")\n",
    "    #print(nested_scores_smote_undersampling[\"indices\"][\"test\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e37a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 11\n",
    "\n",
    "y_true_train = np.asarray(y_outcome)[nested_scores_smote_undersampling[\"indices\"][\"train\"][fold]]\n",
    "y_true_test = np.asarray(y_outcome)[nested_scores_smote_undersampling[\"indices\"][\"test\"][fold]]\n",
    "\n",
    "fitted = nested_scores_smote_undersampling[\"estimator\"][0].fit(X_features_imputed.loc[nested_scores_smote_undersampling[\"indices\"][\"train\"][fold]], y_true_train) #-#=#\n",
    "\n",
    "pred_test = fitted.predict(X_features_imputed.loc[nested_scores_smote_undersampling[\"indices\"][\"test\"][fold]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8accaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"y_pred: {pred_test}\")\n",
    "print(f\"y_true: {y_true_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf857b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find true positive\n",
    "\n",
    "true_positive_index = -1\n",
    "\n",
    "for i in range(len(y_true_test)):\n",
    "    if y_true_test[i] == 1 and pred_test[i] == 1:\n",
    "        true_positive_index = nested_scores_smote_undersampling[\"indices\"][\"test\"][fold][i]\n",
    "        print(f\"true positive at index {true_positive_index}, i={i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222550e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find true negative\n",
    "\n",
    "true_negative_index = -1\n",
    "\n",
    "for i in range(len(y_true_test)):\n",
    "    if y_true_test[i] == 0 and pred_test[i] == 0:\n",
    "        true_negative_index = nested_scores_smote_undersampling[\"indices\"][\"test\"][fold][i]\n",
    "        print(f\"true negative at index {true_negative_index}, i={i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8005551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find false negative\n",
    "\n",
    "false_negative_index = -1\n",
    "\n",
    "for i in range(len(y_true_test)):\n",
    "    if y_true_test[i] == 1 and pred_test[i] == 0:\n",
    "        false_negative_index = nested_scores_smote_undersampling[\"indices\"][\"test\"][fold][i]\n",
    "        print(f\"false negative at index {false_negative_index}, i={i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac840299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find false positive\n",
    "\n",
    "false_positive_index = -1\n",
    "\n",
    "for i in range(len(y_true_test)):\n",
    "    if y_true_test[i] == 0 and pred_test[i] == 1:\n",
    "        false_positive_index = nested_scores_smote_undersampling[\"indices\"][\"test\"][fold][i]\n",
    "        print(f\"false positive at index {false_positive_index}, i={i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data for this individual\n",
    "print(X_volumes_clinical.loc[469])\n",
    "#X_volumes_clinical.loc[329].to_csv(\"patient_inference_results/P0010.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bced28f",
   "metadata": {},
   "source": [
    "#### SHAP explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e5270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitted.best_estimator_[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(fitted.best_estimator_[\"model\"]) \n",
    "shap_values2 = explainer(X_features_imputed.loc[nested_scores_smote_undersampling[\"indices\"][\"test\"][fold]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b446dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values2[94])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e0e66",
   "metadata": {},
   "source": [
    "#### Confidence intervals with test set bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abba49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_list = []\n",
    "\n",
    "for estimator in nested_scores_smote_undersampling[\"estimator\"]:\n",
    "    best_params_list.append(frozenset(estimator.best_params_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab380b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Convert dict to a hashable tuple and count occurrences\n",
    "most_common_params = Counter(best_params_list).most_common(1)[0][0]\n",
    "final_params = dict(most_common_params)  # Convert back to dict\n",
    "\n",
    "\n",
    "print(\"Final chosen hyperparameters:\", final_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf462f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('over', SMOTE(k_neighbors=3, sampling_strategy=0.3)), ('under', RandomUnderSampler(sampling_strategy=0.7)), ('model', HistGradientBoostingClassifier(learning_rate=0.2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b1ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(pipeline, X_features_imputed, y_outcome, cv=20, method='predict_proba')[:,1]\n",
    "\n",
    "y_pred_binary = [1 if i>=0.5 else 0 for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b443e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, confusion_matrix, roc_auc_score, f1_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b68389",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=12345)\n",
    "idx = np.arange(len(y_outcome))\n",
    "\n",
    "y_pred = np.asarray(y_pred)\n",
    "y_pred_binary = np.asarray(y_pred_binary)\n",
    "y = np.asarray(y_outcome)\n",
    "X_dropped = np.asarray(X_features_imputed)\n",
    "\n",
    "test_roc_auc = []\n",
    "test_f1 = []\n",
    "test_ftwos = []\n",
    "test_brier = []\n",
    "test_false_neg = []\n",
    "test_false_pos = []\n",
    "\n",
    "for i in range(200): # bootstrap with 200 rounds: random sampling with replacement of the predictions\n",
    "\n",
    "    pred_idx = rng.choice(idx, size=len(idx), replace=True)\n",
    "    \n",
    "    roc_auc_test_boot = roc_auc_score(y_score=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    f1_test_boot = f1_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx])\n",
    "    f2_test_boot = fbeta_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx], beta=2)\n",
    "    brier_test_boot = brier_score_loss(y_proba=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    false_neg_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[1,0]\n",
    "    false_pos_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[0,1]\n",
    "    \n",
    "    test_roc_auc.append(roc_auc_test_boot)\n",
    "    test_f1.append(f1_test_boot)\n",
    "    test_ftwos.append(f2_test_boot)\n",
    "    test_brier.append(brier_test_boot)\n",
    "    test_false_neg.append(false_neg_test_boot/len(idx)*100)\n",
    "    test_false_pos.append(false_pos_test_boot/len(idx)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde39664",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification performance\\n\")\n",
    "\n",
    "bootstrap_roc_auc_test_mean = np.mean(test_roc_auc)\n",
    "ci_lower = np.percentile(test_roc_auc, 2.5)     # 2.5 percentile (alpha=0.025)\n",
    "ci_upper = np.percentile(test_roc_auc, 97.5)\n",
    "print(f\"ROC AUC:         {bootstrap_roc_auc_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f1_test_mean = np.mean(test_f1)\n",
    "ci_lower = np.percentile(test_f1, 2.5)\n",
    "ci_upper = np.percentile(test_f1, 97.5)\n",
    "print(f\"F1:              {bootstrap_f1_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f2_test_mean = np.mean(test_ftwos)\n",
    "ci_lower = np.percentile(test_ftwos, 2.5)\n",
    "ci_upper = np.percentile(test_ftwos, 97.5)\n",
    "print(f\"F2:              {bootstrap_f2_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_brier_test_mean = np.mean(test_brier)\n",
    "ci_lower = np.percentile(test_brier, 2.5)\n",
    "ci_upper = np.percentile(test_brier, 97.5)\n",
    "print(f\"Brier loss:      {bootstrap_brier_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_neg_test_mean = np.mean(test_false_neg)\n",
    "ci_lower = np.percentile(test_false_neg, 2.5)\n",
    "ci_upper = np.percentile(test_false_neg, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_neg_test_mean:.2f}%;  95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_pos_test_mean = np.mean(test_false_pos)\n",
    "ci_lower = np.percentile(test_false_pos, 2.5)\n",
    "ci_upper = np.percentile(test_false_pos, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_pos_test_mean:.2f}%; 95% CI {ci_lower:.2f}-{ci_upper:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61558cd4-b568-472e-9e2e-6a994a50978e",
   "metadata": {},
   "source": [
    "### Gradient boosting with segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa280aa5-2b82-4300-b55e-431e541d9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run --> prepocessing outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f79fc-2efa-47b0-872c-efed5af7647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(DATA_DIRECTORY+\"segmentation_data_anonymized.csv\", usecols=range(1,16))\n",
    "X.head()\n",
    "\n",
    "y = get_mortality_6m()\n",
    "\n",
    "# Identify indexes where 'mortality' is NaN in y\n",
    "try:\n",
    "    nan_indexes = y.loc[pd.isna(y[\"mortality\"]), :].index\n",
    "except:\n",
    "    nan_indexes=[]\n",
    "print(f\"Indexes with NaN in 'mortality': {nan_indexes}\")\n",
    "\n",
    "# Drop rows with NaN in y\n",
    "y = y.drop(index=nan_indexes)\n",
    "\n",
    "# Ensure 'name' is the index or used for alignment\n",
    "X = X[~X['name'].isin(y.loc[nan_indexes, 'name'])]\n",
    "\n",
    "# Verify the shapes after cleaning\n",
    "print(f\"Shape of y after cleaning: {y.shape}\")\n",
    "print(f\"Shape of X after cleaning: {X.shape}\")\n",
    "\n",
    "# VÃ©rifiez les valeurs uniques de 'name' dans y\n",
    "print(\"Unique 'name' values in y:\", y['name'].nunique())\n",
    "\n",
    "# VÃ©rifiez les valeurs uniques de 'name' dans X\n",
    "print(\"Unique 'name' values in X:\", X['name'].nunique())\n",
    "\n",
    "# Noms dans y mais pas dans X\n",
    "missing_in_X = set(y['name']) - set(X['name'])\n",
    "print(f\"Names in y but not in X: {missing_in_X}\")\n",
    "\n",
    "# Noms dans X mais pas dans y\n",
    "missing_in_y = set(X['name']) - set(y['name'])\n",
    "print(f\"Names in X but not in y: {missing_in_y}\")\n",
    "\n",
    "# Trouver les noms communs\n",
    "common_names = set(y['name']).intersection(set(X['name']))\n",
    "\n",
    "# Filtrer y et X pour ne garder que les noms communs\n",
    "y = y[y['name'].isin(common_names)]\n",
    "X = X[X['name'].isin(common_names)]\n",
    "\n",
    "# VÃ©rifiez les dimensions aprÃ¨s nettoyage\n",
    "print(f\"Shape of y after alignment: {y.shape}\")\n",
    "print(f\"Shape of X after alignment: {X.shape}\")\n",
    "\n",
    "# Check for duplicates in 'name' in y\n",
    "print(\"Number of duplicate names in y:\", y['name'].duplicated().sum())\n",
    "\n",
    "# Check for duplicates in 'name' in X\n",
    "print(\"Number of duplicate names in X:\", X['name'].duplicated().sum())\n",
    "\n",
    "# Remove duplicates from both DataFrames\n",
    "y = y.drop_duplicates(subset=['name'])\n",
    "X = X.drop_duplicates(subset=['name'])\n",
    "\n",
    "# Verify the shapes again\n",
    "print(f\"Shape of y after removing duplicates: {y.shape}\")\n",
    "print(f\"Shape of X after removing duplicates: {X.shape}\")\n",
    "\n",
    "# Check if 'name' values are the same in both DataFrames\n",
    "common_names = set(y['name']) == set(X['name'])\n",
    "print(f\"Are 'name' values aligned between y and X {common_names}\")\n",
    "\n",
    "# Trier y et X par la colonne 'name'\n",
    "y = y.sort_values(by='name').reset_index(drop=True)\n",
    "X = X.sort_values(by='name').reset_index(drop=True)\n",
    "\n",
    "# VÃ©rifier si les noms sont alignÃ©s\n",
    "print((y['name'].values == X['name'].values).all())\n",
    "\n",
    "# Drop the 'name' column from X\n",
    "X_features = X.drop(columns=['name'])\n",
    "\n",
    "# imputation with median \n",
    "X_features_imputed = X_features.fillna(X_features.median())\n",
    "\n",
    "# Drop the 'name' column from y (if it exists)\n",
    "y_outcome = y.drop(columns=['name'])\n",
    "\n",
    "# Ensure y_outcome is a 1D array\n",
    "y_outcome = y_outcome.values.ravel()  # Convert to 1D if using pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30e794-21db-4faf-828d-383fef042336",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ones = np.sum(y_outcome == 1)  # True values are treated as 1 in numpy\n",
    "print(f\"Number of y = 1 in y_outcome: {num_ones}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c90a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064cad9-f62f-4af1-9225-b657e5b7bc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "N_REPEATS = 3\n",
    "nb_total_samples = len(y_outcome)\n",
    "\n",
    "pipeline_smote_under = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf = GridSearchCV(estimator=pipeline_smote_under, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv)\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling = cross_validate(clf, X_features_imputed, y_outcome, \n",
    "                                                   scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, \n",
    "                                                   cv=outer_cv, n_jobs=-1, return_estimator=True, return_indices=True)\n",
    "\n",
    "print(\"HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3414cb4f",
   "metadata": {},
   "source": [
    "#### Confidence intervals with test set bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf61a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_list = []\n",
    "\n",
    "for estimator in nested_scores_smote_undersampling[\"estimator\"]:\n",
    "    best_params_list.append(frozenset(estimator.best_params_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f602ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Convert dict to a hashable tuple and count occurrences\n",
    "most_common_params = Counter(best_params_list).most_common(1)[0][0]\n",
    "final_params = dict(most_common_params)  # Convert back to dict\n",
    "\n",
    "\n",
    "print(\"Final chosen hyperparameters:\", final_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34818bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('over', SMOTE(k_neighbors=3, sampling_strategy=0.3)), ('under', RandomUnderSampler(sampling_strategy=0.7)), ('model', HistGradientBoostingClassifier(learning_rate=0.2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f72859",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(pipeline, X_features_imputed, y_outcome, cv=20, method='predict_proba')[:,1]\n",
    "\n",
    "y_pred_binary = [1 if i>=0.5 else 0 for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ef0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, confusion_matrix, roc_auc_score, f1_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=12345)\n",
    "idx = np.arange(len(y_outcome))\n",
    "\n",
    "y_pred = np.asarray(y_pred)\n",
    "y_pred_binary = np.asarray(y_pred_binary)\n",
    "y = np.asarray(y_outcome)\n",
    "X_dropped = np.asarray(X_features_imputed)\n",
    "\n",
    "test_roc_auc = []\n",
    "test_f1 = []\n",
    "test_ftwos = []\n",
    "test_brier = []\n",
    "test_false_neg = []\n",
    "test_false_pos = []\n",
    "\n",
    "for i in range(200): # bootstrap with 200 rounds: random sampling with replacement of the predictions\n",
    "\n",
    "    pred_idx = rng.choice(idx, size=len(idx), replace=True)\n",
    "    \n",
    "    roc_auc_test_boot = roc_auc_score(y_score=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    f1_test_boot = f1_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx])\n",
    "    f2_test_boot = fbeta_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx], beta=2)\n",
    "    brier_test_boot = brier_score_loss(y_proba=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    false_neg_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[1,0]\n",
    "    false_pos_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[0,1]\n",
    "    \n",
    "    test_roc_auc.append(roc_auc_test_boot)\n",
    "    test_f1.append(f1_test_boot)\n",
    "    test_ftwos.append(f2_test_boot)\n",
    "    test_brier.append(brier_test_boot)\n",
    "    test_false_neg.append(false_neg_test_boot/len(idx)*100)\n",
    "    test_false_pos.append(false_pos_test_boot/len(idx)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification performance\\n\")\n",
    "\n",
    "bootstrap_roc_auc_test_mean = np.mean(test_roc_auc)\n",
    "ci_lower = np.percentile(test_roc_auc, 2.5)     # 2.5 percentile (alpha=0.025)\n",
    "ci_upper = np.percentile(test_roc_auc, 97.5)\n",
    "print(f\"ROC AUC:         {bootstrap_roc_auc_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f1_test_mean = np.mean(test_f1)\n",
    "ci_lower = np.percentile(test_f1, 2.5)\n",
    "ci_upper = np.percentile(test_f1, 97.5)\n",
    "print(f\"F1:              {bootstrap_f1_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f2_test_mean = np.mean(test_ftwos)\n",
    "ci_lower = np.percentile(test_ftwos, 2.5)\n",
    "ci_upper = np.percentile(test_ftwos, 97.5)\n",
    "print(f\"F2:              {bootstrap_f2_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_brier_test_mean = np.mean(test_brier)\n",
    "ci_lower = np.percentile(test_brier, 2.5)\n",
    "ci_upper = np.percentile(test_brier, 97.5)\n",
    "print(f\"Brier loss:      {bootstrap_brier_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_neg_test_mean = np.mean(test_false_neg)\n",
    "ci_lower = np.percentile(test_false_neg, 2.5)\n",
    "ci_upper = np.percentile(test_false_neg, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_neg_test_mean:.2f}%;  95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_pos_test_mean = np.mean(test_false_pos)\n",
    "ci_lower = np.percentile(test_false_pos, 2.5)\n",
    "ci_upper = np.percentile(test_false_pos, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_pos_test_mean:.2f}%; 95% CI {ci_lower:.2f}-{ci_upper:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d884295",
   "metadata": {},
   "source": [
    "#### Get individual test set samples prediction vs ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "pred = nested_scores_smote_undersampling[\"estimator\"][0].predict(X_features_imputed.loc[nested_scores_smote_undersampling[\"indices\"][\"test\"][fold]])\n",
    "y_true = np.asarray(y_outcome)[nested_scores_smote_undersampling[\"indices\"][\"test\"][fold]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"y_pred: {pred}\")\n",
    "print(f\"y_true: {y_true}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find false positive\n",
    "\n",
    "false_negative_index = -1\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i] == 1 and pred[i] == 0:\n",
    "        false_negative_index = nested_scores_smote_undersampling[\"indices\"][\"test\"][fold][i]\n",
    "        print(f\"false negative at index {false_negative_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487cf6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data for this individual\n",
    "print(X.loc[false_negative_index])\n",
    "X.loc[false_negative_index].to_csv(\"patient_inference_results/P0501.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find false positive\n",
    "\n",
    "false_positive_index = -1\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i] == 0 and pred[i] == 1:\n",
    "        false_positive_index = nested_scores_smote_undersampling[\"indices\"][\"test\"][fold][i]\n",
    "        print(f\"false positive at index {false_positive_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8062b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data for this individual\n",
    "print(X.loc[false_positive_index])\n",
    "#X.loc[false_positive_index].to_csv(\"patient_inference_results/P0498.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a339e7-2ba2-4d5f-a8b8-149d6cb6bd28",
   "metadata": {},
   "source": [
    "### Gradient boosting with traumatrix model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af020a91-8d14-47db-9581-eb251ca139fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(DATA_DIRECTORY+\"segmentation_data_anonymized.csv\", usecols=[1]+list(range(16,31)))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5adc3-822b-47f8-bf11-f6bd0d7ee133",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = get_mortality_6m()\n",
    "\n",
    "# Identify indexes where 'mortality' is NaN in y\n",
    "try:\n",
    "    nan_indexes = y.loc[pd.isna(y[\"mortality\"]), :].index\n",
    "except:\n",
    "    nan_indexes=[]\n",
    "print(f\"Indexes with NaN in 'mortality': {nan_indexes}\")\n",
    "\n",
    "# Drop rows with NaN in y\n",
    "y = y.drop(index=nan_indexes)\n",
    "\n",
    "# Ensure 'name' is the index or used for alignment\n",
    "X = X[~X['name'].isin(y.loc[nan_indexes, 'name'])]\n",
    "\n",
    "# Verify the shapes after cleaning\n",
    "print(f\"Shape of y after cleaning: {y.shape}\")\n",
    "print(f\"Shape of X after cleaning: {X.shape}\")\n",
    "\n",
    "# VÃ©rifiez les valeurs uniques de 'name' dans y\n",
    "print(\"Unique 'name' values in y:\", y['name'].nunique())\n",
    "\n",
    "# VÃ©rifiez les valeurs uniques de 'name' dans X\n",
    "print(\"Unique 'name' values in X:\", X['name'].nunique())\n",
    "\n",
    "# Noms dans y mais pas dans X\n",
    "missing_in_X = set(y['name']) - set(X['name'])\n",
    "print(f\"Names in y but not in X: {missing_in_X}\")\n",
    "\n",
    "# Noms dans X mais pas dans y\n",
    "missing_in_y = set(X['name']) - set(y['name'])\n",
    "print(f\"Names in X but not in y: {missing_in_y}\")\n",
    "\n",
    "# Trouver les noms communs\n",
    "common_names = set(y['name']).intersection(set(X['name']))\n",
    "\n",
    "# Filtrer y et X pour ne garder que les noms communs\n",
    "y = y[y['name'].isin(common_names)]\n",
    "X = X[X['name'].isin(common_names)]\n",
    "\n",
    "# VÃ©rifiez les dimensions aprÃ¨s nettoyage\n",
    "print(f\"Shape of y after alignment: {y.shape}\")\n",
    "print(f\"Shape of X after alignment: {X.shape}\")\n",
    "\n",
    "# Check for duplicates in 'name' in y\n",
    "print(\"Number of duplicate names in y:\", y['name'].duplicated().sum())\n",
    "\n",
    "# Check for duplicates in 'name' in X\n",
    "print(\"Number of duplicate names in X:\", X['name'].duplicated().sum())\n",
    "\n",
    "# Remove duplicates from both DataFrames\n",
    "y = y.drop_duplicates(subset=['name'])\n",
    "X = X.drop_duplicates(subset=['name'])\n",
    "\n",
    "# Verify the shapes again\n",
    "print(f\"Shape of y after removing duplicates: {y.shape}\")\n",
    "print(f\"Shape of X after removing duplicates: {X.shape}\")\n",
    "\n",
    "# Check if 'name' values are the same in both DataFrames\n",
    "common_names = set(y['name']) == set(X['name'])\n",
    "print(f\"Are 'name' values aligned between y and X {common_names}\")\n",
    "\n",
    "# Trier y et X par la colonne 'name'\n",
    "y = y.sort_values(by='name').reset_index(drop=True)\n",
    "X = X.sort_values(by='name').reset_index(drop=True)\n",
    "\n",
    "# VÃ©rifier si les noms sont alignÃ©s\n",
    "print((y['name'].values == X['name'].values).all())\n",
    "\n",
    "# Drop the 'name' column from X\n",
    "X_features = X.drop(columns=['name'])\n",
    "\n",
    "# imputation with median \n",
    "X_features_imputed = X_features.fillna(X_features.median())\n",
    "\n",
    "# Drop the 'name' column from y (if it exists)\n",
    "y_outcome = y.drop(columns=['name'])\n",
    "\n",
    "# Ensure y_outcome is a 1D array\n",
    "y_outcome = y_outcome.values.ravel()  # Convert to 1D if using pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb3d120-befe-4743-bee7-e49128c254c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_smote_under_traumatrix = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv_traumatrix = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf_traumatrix = GridSearchCV(estimator=pipeline_smote_under_traumatrix, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv_traumatrix)\n",
    "\n",
    "outer_cv_traumatrix = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling_traumatrix = cross_validate(clf_traumatrix, X_features_imputed, y_outcome, \n",
    "                                                   scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, \n",
    "                                                   cv=outer_cv_traumatrix, n_jobs=-1, return_estimator=True, return_indices=True)\n",
    "\n",
    "print(\"HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling_traumatrix[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling_traumatrix[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling_traumatrix[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling_traumatrix[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling_traumatrix[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling_traumatrix[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling_traumatrix[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling_traumatrix[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling_traumatrix[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling_traumatrix[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling_traumatrix[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling_traumatrix[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe30b5",
   "metadata": {},
   "source": [
    "#### Get individual test set samples prediction vs ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5391cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 2\n",
    "\n",
    "y_true_train_traumatrix = np.asarray(y_outcome)[nested_scores_smote_undersampling_traumatrix[\"indices\"][\"train\"][fold]]\n",
    "y_true_test_traumatrix = np.asarray(y_outcome)[nested_scores_smote_undersampling_traumatrix[\"indices\"][\"test\"][fold]]\n",
    "\n",
    "fitted_traumatrix = nested_scores_smote_undersampling_traumatrix[\"estimator\"][0].fit(X_features_imputed.loc[nested_scores_smote_undersampling_traumatrix[\"indices\"][\"train\"][fold]], y_true_train) #-#=#\n",
    "\n",
    "pred_test_traumatrix = fitted_traumatrix.predict(X_features_imputed.loc[nested_scores_smote_undersampling_traumatrix[\"indices\"][\"test\"][fold]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"y_pred: {pred_test_traumatrix}\")\n",
    "print(f\"y_true: {y_true_test_traumatrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71dc6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find true positive\n",
    "\n",
    "true_positive_index_traumatrix = -1\n",
    "\n",
    "for i in range(len(y_true_test_traumatrix)):\n",
    "    if y_true_test_traumatrix[i] == 1 and pred_test_traumatrix[i] == 1:\n",
    "        true_positive_index_traumatrix = nested_scores_smote_undersampling[\"indices\"][\"test\"][fold][i]\n",
    "        print(f\"true positive at index {true_positive_index_traumatrix}, i={i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find true negative\n",
    "\n",
    "true_negative_index_traumatrix = -1\n",
    "\n",
    "for i in range(len(y_true_test_traumatrix)):\n",
    "    if y_true_test_traumatrix[i] == 0 and pred_test_traumatrix[i] == 0:\n",
    "        true_negative_index_traumatrix = nested_scores_smote_undersampling_traumatrix[\"indices\"][\"test\"][fold][i]\n",
    "        print(f\"true negative at index {true_negative_index_traumatrix}, i={i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find false negative\n",
    "\n",
    "false_negative_index_traumatrix = -1\n",
    "\n",
    "for i in range(len(y_true_test_traumatrix)):\n",
    "    if y_true_test_traumatrix[i] == 1 and pred_test_traumatrix[i] == 0:\n",
    "        false_negative_index_traumatrix = nested_scores_smote_undersampling_traumatrix[\"indices\"][\"test\"][fold][i]\n",
    "        print(f\"false negative at index {false_negative_index_traumatrix}, i={i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa30f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find false positive\n",
    "\n",
    "false_positive_index_traumatrix = -1\n",
    "\n",
    "for i in range(len(y_true_test_traumatrix)):\n",
    "    if y_true_test_traumatrix[i] == 0 and pred_test_traumatrix[i] == 1:\n",
    "        false_positive_index_traumatrix = nested_scores_smote_undersampling_traumatrix[\"indices\"][\"test\"][fold][i]\n",
    "        print(f\"false positive at index {false_positive_index_traumatrix}, i={i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data for this individual\n",
    "print(X_volumes_clinical.loc[24])\n",
    "#X_volumes_clinical.loc[329].to_csv(\"patient_inference_results/P0010.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd83c2",
   "metadata": {},
   "source": [
    "#### SHAP explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6889d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitted.best_estimator_[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(fitted.best_estimator_[\"model\"]) \n",
    "shap_values2 = explainer(X_features_imputed.loc[nested_scores_smote_undersampling[\"indices\"][\"test\"][fold]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4cd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a5a8f4",
   "metadata": {},
   "source": [
    "#### Confidence intervals with test set bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_list = []\n",
    "\n",
    "for estimator in nested_scores_smote_undersampling[\"estimator\"]:\n",
    "    best_params_list.append(frozenset(estimator.best_params_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db885c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Convert dict to a hashable tuple and count occurrences\n",
    "most_common_params = Counter(best_params_list).most_common(1)[0][0]\n",
    "final_params = dict(most_common_params)  # Convert back to dict\n",
    "\n",
    "\n",
    "print(\"Final chosen hyperparameters:\", final_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f60b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('over', SMOTE(k_neighbors=3, sampling_strategy=0.3)), ('under', RandomUnderSampler(sampling_strategy=0.7)), ('model', HistGradientBoostingClassifier(learning_rate=0.2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(pipeline, X_features_imputed, y_outcome, cv=20, method='predict_proba')[:,1]\n",
    "\n",
    "y_pred_binary = [1 if i>=0.5 else 0 for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46550f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, confusion_matrix, roc_auc_score, f1_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bc1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=12345)\n",
    "idx = np.arange(len(y_outcome))\n",
    "\n",
    "y_pred = np.asarray(y_pred)\n",
    "y_pred_binary = np.asarray(y_pred_binary)\n",
    "y = np.asarray(y_outcome)\n",
    "X_dropped = np.asarray(X_features_imputed)\n",
    "\n",
    "test_roc_auc = []\n",
    "test_f1 = []\n",
    "test_ftwos = []\n",
    "test_brier = []\n",
    "test_false_neg = []\n",
    "test_false_pos = []\n",
    "\n",
    "for i in range(200): # bootstrap with 200 rounds: random sampling with replacement of the predictions\n",
    "\n",
    "    pred_idx = rng.choice(idx, size=len(idx), replace=True)\n",
    "    \n",
    "    roc_auc_test_boot = roc_auc_score(y_score=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    f1_test_boot = f1_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx])\n",
    "    f2_test_boot = fbeta_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx], beta=2)\n",
    "    brier_test_boot = brier_score_loss(y_proba=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    false_neg_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[1,0]\n",
    "    false_pos_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[0,1]\n",
    "    \n",
    "    test_roc_auc.append(roc_auc_test_boot)\n",
    "    test_f1.append(f1_test_boot)\n",
    "    test_ftwos.append(f2_test_boot)\n",
    "    test_brier.append(brier_test_boot)\n",
    "    test_false_neg.append(false_neg_test_boot/len(idx)*100)\n",
    "    test_false_pos.append(false_pos_test_boot/len(idx)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification performance\\n\")\n",
    "\n",
    "bootstrap_roc_auc_test_mean = np.mean(test_roc_auc)\n",
    "ci_lower = np.percentile(test_roc_auc, 2.5)     # 2.5 percentile (alpha=0.025)\n",
    "ci_upper = np.percentile(test_roc_auc, 97.5)\n",
    "print(f\"ROC AUC:         {bootstrap_roc_auc_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f1_test_mean = np.mean(test_f1)\n",
    "ci_lower = np.percentile(test_f1, 2.5)\n",
    "ci_upper = np.percentile(test_f1, 97.5)\n",
    "print(f\"F1:              {bootstrap_f1_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f2_test_mean = np.mean(test_ftwos)\n",
    "ci_lower = np.percentile(test_ftwos, 2.5)\n",
    "ci_upper = np.percentile(test_ftwos, 97.5)\n",
    "print(f\"F2:              {bootstrap_f2_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_brier_test_mean = np.mean(test_brier)\n",
    "ci_lower = np.percentile(test_brier, 2.5)\n",
    "ci_upper = np.percentile(test_brier, 97.5)\n",
    "print(f\"Brier loss:      {bootstrap_brier_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_neg_test_mean = np.mean(test_false_neg)\n",
    "ci_lower = np.percentile(test_false_neg, 2.5)\n",
    "ci_upper = np.percentile(test_false_neg, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_neg_test_mean:.2f}%;  95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_pos_test_mean = np.mean(test_false_pos)\n",
    "ci_lower = np.percentile(test_false_pos, 2.5)\n",
    "ci_upper = np.percentile(test_false_pos, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_pos_test_mean:.2f}%; 95% CI {ci_lower:.2f}-{ci_upper:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926f42f",
   "metadata": {},
   "source": [
    "#### Confidence intervals with test set bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_list_traumatrix = []\n",
    "\n",
    "for estimator in nested_scores_smote_undersampling_traumatrix[\"estimator\"]:\n",
    "    best_params_list_traumatrix.append(frozenset(estimator.best_params_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542acbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Convert dict to a hashable tuple and count occurrences\n",
    "most_common_params_traumatrix = Counter(best_params_list_traumatrix).most_common(1)[0][0]\n",
    "final_params_traumatrix = dict(most_common_params_traumatrix)  # Convert back to dict\n",
    "\n",
    "\n",
    "print(\"Final chosen hyperparameters:\", final_params_traumatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db86213",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_traumatrix = Pipeline(steps=[('over', SMOTE(k_neighbors=8, sampling_strategy=0.2)), ('under', RandomUnderSampler(sampling_strategy=0.7)), ('model', HistGradientBoostingClassifier(learning_rate=0.08))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_traumatrix = cross_val_predict(pipeline_traumatrix, X_features_imputed, y_outcome, cv=20, method='predict_proba')[:,1]\n",
    "\n",
    "y_pred_binary_traumatrix = [1 if i>=0.5 else 0 for i in y_pred_traumatrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d95795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, confusion_matrix, roc_auc_score, f1_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20353d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=12345)\n",
    "idx = np.arange(len(y_outcome))\n",
    "\n",
    "y_pred_traumatrix = np.asarray(y_pred_traumatrix)\n",
    "y_pred_binary_traumatrix = np.asarray(y_pred_binary_traumatrix)\n",
    "y = np.asarray(y_outcome)\n",
    "X_dropped = np.asarray(X_features_imputed)\n",
    "\n",
    "test_roc_auc = []\n",
    "test_f1 = []\n",
    "test_ftwos = []\n",
    "test_brier = []\n",
    "test_false_neg = []\n",
    "test_false_pos = []\n",
    "\n",
    "for i in range(200): # bootstrap with 200 rounds: random sampling with replacement of the predictions\n",
    "\n",
    "    pred_idx = rng.choice(idx, size=len(idx), replace=True)\n",
    "    \n",
    "    roc_auc_test_boot = roc_auc_score(y_score=y_pred_traumatrix[pred_idx], y_true=y[pred_idx])\n",
    "    f1_test_boot = f1_score(y_pred=y_pred_binary_traumatrix[pred_idx], y_true=y[pred_idx])\n",
    "    f2_test_boot = fbeta_score(y_pred=y_pred_binary_traumatrix[pred_idx], y_true=y[pred_idx], beta=2)\n",
    "    brier_test_boot = brier_score_loss(y_proba=y_pred_traumatrix[pred_idx], y_true=y[pred_idx])\n",
    "    false_neg_test_boot = confusion_matrix(y[pred_idx], y_pred_binary_traumatrix[pred_idx])[1,0]\n",
    "    false_pos_test_boot = confusion_matrix(y[pred_idx], y_pred_binary_traumatrix[pred_idx])[0,1]\n",
    "    \n",
    "    test_roc_auc.append(roc_auc_test_boot)\n",
    "    test_f1.append(f1_test_boot)\n",
    "    test_ftwos.append(f2_test_boot)\n",
    "    test_brier.append(brier_test_boot)\n",
    "    test_false_neg.append(false_neg_test_boot/len(idx)*100)\n",
    "    test_false_pos.append(false_pos_test_boot/len(idx)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de4077",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification performance\\n\")\n",
    "\n",
    "bootstrap_roc_auc_test_mean = np.mean(test_roc_auc)\n",
    "ci_lower = np.percentile(test_roc_auc, 2.5)     # 2.5 percentile (alpha=0.025)\n",
    "ci_upper = np.percentile(test_roc_auc, 97.5)\n",
    "print(f\"ROC AUC:         {bootstrap_roc_auc_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f1_test_mean = np.mean(test_f1)\n",
    "ci_lower = np.percentile(test_f1, 2.5)\n",
    "ci_upper = np.percentile(test_f1, 97.5)\n",
    "print(f\"F1:              {bootstrap_f1_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f2_test_mean = np.mean(test_ftwos)\n",
    "ci_lower = np.percentile(test_ftwos, 2.5)\n",
    "ci_upper = np.percentile(test_ftwos, 97.5)\n",
    "print(f\"F2:              {bootstrap_f2_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_brier_test_mean = np.mean(test_brier)\n",
    "ci_lower = np.percentile(test_brier, 2.5)\n",
    "ci_upper = np.percentile(test_brier, 97.5)\n",
    "print(f\"Brier loss:      {bootstrap_brier_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_neg_test_mean = np.mean(test_false_neg)\n",
    "ci_lower = np.percentile(test_false_neg, 2.5)\n",
    "ci_upper = np.percentile(test_false_neg, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_neg_test_mean:.2f}%;  95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_pos_test_mean = np.mean(test_false_pos)\n",
    "ci_lower = np.percentile(test_false_pos, 2.5)\n",
    "ci_upper = np.percentile(test_false_pos, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_pos_test_mean:.2f}%; 95% CI {ci_lower:.2f}-{ci_upper:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb29faa-0b5a-473d-bebc-bacc6b45dd60",
   "metadata": {},
   "source": [
    "### Gradient boosting with all prehospital features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5afc49d2-47e6-4eab-ab8b-3f9c0fb8cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run preprocessing outcome mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ac410d-f94e-4ff9-92a9-29bb34fb3c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>name</th>\n",
       "      <th>Date Naissance</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sexe</th>\n",
       "      <th>Venue</th>\n",
       "      <th>EntrÃ©e venue</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Exclusion</th>\n",
       "      <th>...</th>\n",
       "      <th>score_glasgow_initial</th>\n",
       "      <th>score_glasgow_moteur_initial</th>\n",
       "      <th>anomalie_pupillaire_prehospitalier</th>\n",
       "      <th>frequence_cardiaque_FC_arrivee_du_smur</th>\n",
       "      <th>arret_cardio_respiratoire_massage</th>\n",
       "      <th>penetrant_objet</th>\n",
       "      <th>ischemie_du_membre</th>\n",
       "      <th>hemorragie_externe</th>\n",
       "      <th>amputation</th>\n",
       "      <th>outcome_neurochir_pic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P0000</td>\n",
       "      <td>11/19/1941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2.000010e+12</td>\n",
       "      <td>8/5/2020 21:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>P0001</td>\n",
       "      <td>11/21/1968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2.000010e+12</td>\n",
       "      <td>8/6/2020 11:50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>P0002</td>\n",
       "      <td>6/14/1997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2.000010e+12</td>\n",
       "      <td>8/7/2020 21:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>P0003</td>\n",
       "      <td>8/5/1978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>2.000010e+12</td>\n",
       "      <td>8/8/2020 19:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>P0004</td>\n",
       "      <td>11/17/1986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2.000010e+12</td>\n",
       "      <td>8/9/2020 2:19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0_x   name Date Naissance  Age Sexe         Venue  \\\n",
       "0           0             0  P0000     11/19/1941  NaN    M  2.000010e+12   \n",
       "1           1             1  P0001     11/21/1968  NaN    M  2.000010e+12   \n",
       "2           2             2  P0002      6/14/1997  NaN    M  2.000010e+12   \n",
       "3           3             3  P0003       8/5/1978  NaN    F  2.000010e+12   \n",
       "4           4             4  P0004     11/17/1986  NaN    M  2.000010e+12   \n",
       "\n",
       "     EntrÃ©e venue  Unnamed: 9 Exclusion  ... score_glasgow_initial  \\\n",
       "0  8/5/2020 21:36         NaN       NaN  ...                  15.0   \n",
       "1  8/6/2020 11:50         NaN       NaN  ...                  15.0   \n",
       "2  8/7/2020 21:31         NaN       NaN  ...                  15.0   \n",
       "3  8/8/2020 19:57         NaN       NaN  ...                  14.0   \n",
       "4   8/9/2020 2:19         NaN       NaN  ...                  15.0   \n",
       "\n",
       "  score_glasgow_moteur_initial anomalie_pupillaire_prehospitalier  \\\n",
       "0                          6.0                                0.0   \n",
       "1                          6.0                                0.0   \n",
       "2                          6.0                                0.0   \n",
       "3                          6.0                                0.0   \n",
       "4                          6.0                                0.0   \n",
       "\n",
       "   frequence_cardiaque_FC_arrivee_du_smur arret_cardio_respiratoire_massage  \\\n",
       "0                                   137.0                               0.0   \n",
       "1                                    56.0                               0.0   \n",
       "2                                   100.0                               0.0   \n",
       "3                                   120.0                               0.0   \n",
       "4                                   107.0                               0.0   \n",
       "\n",
       "  penetrant_objet ischemie_du_membre hemorragie_externe amputation  \\\n",
       "0             0.0                0.0                0.0        0.0   \n",
       "1             0.0                0.0                0.0        0.0   \n",
       "2             0.0                0.0                0.0        0.0   \n",
       "3             0.0                0.0                0.0        0.0   \n",
       "4             0.0                0.0                0.0        0.0   \n",
       "\n",
       "  outcome_neurochir_pic  \n",
       "0                   0.0  \n",
       "1                   0.0  \n",
       "2                   0.0  \n",
       "3                   0.0  \n",
       "4                   0.0  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_full = pd.read_csv(DATA_DIRECTORY+\"final_database_clinical_segmentation.csv\")\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b59844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns 16 to 36\n",
    "X_prehosp = data_full.iloc[:, 15:35]\n",
    "\n",
    "# Convert all columns to numeric, replacing non-numeric values and empty strings with NaN\n",
    "X_prehosp = X_prehosp.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Include column name\n",
    "X_prehosp = pd.concat([X_prehosp, data_full.iloc[:, 2]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f19d33c-379d-48a5-bbd6-e1a8886d5ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values per column:\n",
      "PAS  SMUR                   40\n",
      "PAD  SMUR                   49\n",
      "FC SMUR                     42\n",
      "FR SMUR                    485\n",
      "Shock Index SMUR            55\n",
      "GCS SMUR                    17\n",
      "GCS (M) SMUR                30\n",
      "HÃ©mocue SMUR               243\n",
      "Shock Index inversÃ©         54\n",
      "Shock index diastolique     62\n",
      "Anomalie pupille SMUR       17\n",
      "Fracas bassin               14\n",
      "Amputation                  12\n",
      "ACR SMUR                    11\n",
      "HÃ©morragie ext SMUR         12\n",
      "IschÃ©mie                    12\n",
      "Intubation prehosp          11\n",
      "Expansion volÃ©mique         20\n",
      "OsmoTH prehosp              11\n",
      "Vasopresseur prehosp        12\n",
      "name                         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAS  SMUR</th>\n",
       "      <th>PAD  SMUR</th>\n",
       "      <th>FC SMUR</th>\n",
       "      <th>FR SMUR</th>\n",
       "      <th>Shock Index SMUR</th>\n",
       "      <th>GCS SMUR</th>\n",
       "      <th>GCS (M) SMUR</th>\n",
       "      <th>HÃ©mocue SMUR</th>\n",
       "      <th>Shock Index inversÃ©</th>\n",
       "      <th>Shock index diastolique</th>\n",
       "      <th>...</th>\n",
       "      <th>Fracas bassin</th>\n",
       "      <th>Amputation</th>\n",
       "      <th>ACR SMUR</th>\n",
       "      <th>HÃ©morragie ext SMUR</th>\n",
       "      <th>IschÃ©mie</th>\n",
       "      <th>Intubation prehosp</th>\n",
       "      <th>Expansion volÃ©mique</th>\n",
       "      <th>OsmoTH prehosp</th>\n",
       "      <th>Vasopresseur prehosp</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.72</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.51</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P0004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PAS  SMUR   PAD  SMUR   FC SMUR   FR SMUR  Shock Index SMUR  GCS SMUR   \\\n",
       "0       190.0       103.0     137.0      NaN              0.72       15.0   \n",
       "1        87.0        49.0      56.0      NaN              0.64       15.0   \n",
       "2       100.0        60.0     100.0     17.0              1.00       15.0   \n",
       "3       101.0        64.0     120.0      NaN              1.19       14.0   \n",
       "4       110.0        71.0     107.0     18.0              0.97       15.0   \n",
       "\n",
       "   GCS (M) SMUR   HÃ©mocue SMUR   Shock Index inversÃ©  Shock index diastolique  \\\n",
       "0            6.0            NaN                 1.39                     1.33   \n",
       "1            6.0            NaN                 1.55                     1.14   \n",
       "2            6.0            NaN                 1.00                     1.67   \n",
       "3            6.0           13.1                 0.84                     1.88   \n",
       "4            6.0           15.8                 1.03                     1.51   \n",
       "\n",
       "   ...  Fracas bassin  Amputation  ACR SMUR  HÃ©morragie ext SMUR  IschÃ©mie  \\\n",
       "0  ...            0.0         0.0       0.0                  0.0       0.0   \n",
       "1  ...            0.0         0.0       0.0                  0.0       0.0   \n",
       "2  ...            0.0         0.0       0.0                  0.0       0.0   \n",
       "3  ...            0.0         0.0       0.0                  0.0       0.0   \n",
       "4  ...            0.0         0.0       0.0                  0.0       0.0   \n",
       "\n",
       "   Intubation prehosp  Expansion volÃ©mique  OsmoTH prehosp  \\\n",
       "0                 0.0                500.0             0.0   \n",
       "1                 0.0                250.0             0.0   \n",
       "2                 0.0                250.0             0.0   \n",
       "3                 0.0                250.0             0.0   \n",
       "4                 0.0                500.0             0.0   \n",
       "\n",
       "   Vasopresseur prehosp   name  \n",
       "0                   0.0  P0000  \n",
       "1                   0.0  P0001  \n",
       "2                   0.0  P0002  \n",
       "3                   0.0  P0003  \n",
       "4                   0.0  P0004  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Verify if there are NaN values\n",
    "print(f\"Number of NaN values per column:\\n{X_prehosp.isna().sum()}\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "X_prehosp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b908a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_prehosp.copy()\n",
    "y = get_mortality_6m()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8412e1-1a5b-4602-8cff-cef01799c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes with NaN in 'mortality': Index([], dtype='int64')\n",
      "Shape of y after cleaning: (566, 2)\n",
      "Shape of X after cleaning: (534, 21)\n",
      "Unique 'name' values in y: 562\n",
      "Unique 'name' values in X: 534\n",
      "Names in y but not in X: {'P0349', 'P0518', 'P0155', 'P0150', 'P0383', 'P0224', 'P0217', 'P0564', 'P0532', 'P0327', 'P0147', 'P0207', 'P0018', 'P0195', 'P0430', 'P0239', 'P0149', 'P0480', 'P0083', 'P0369', 'P0422', 'P0412', 'P0111', 'P0356', 'P0010', 'P0527', 'P0343', 'P0491'}\n",
      "Names in X but not in y: set()\n",
      "Shape of y after alignment: (536, 2)\n",
      "Shape of X after alignment: (534, 21)\n",
      "Number of duplicate names in y: 2\n",
      "Number of duplicate names in X: 0\n",
      "Shape of y after removing duplicates: (534, 2)\n",
      "Shape of X after removing duplicates: (534, 21)\n",
      "Are 'name' values aligned between y and X True\n"
     ]
    }
   ],
   "source": [
    "# Identify indexes where 'mortality' is NaN in y\n",
    "nan_indexes = y.loc[pd.isna(y[\"mortality\"]), :].index\n",
    "print(f\"Indexes with NaN in 'mortality': {nan_indexes}\")\n",
    "\n",
    "# Drop rows with NaN in y\n",
    "y = y.drop(index=nan_indexes)\n",
    "\n",
    "# Ensure 'name' is the index or used for alignment\n",
    "X = X[~X['name'].isin(y.loc[nan_indexes, 'name'])]\n",
    "\n",
    "# Verify the shapes after cleaning\n",
    "print(f\"Shape of y after cleaning: {y.shape}\")\n",
    "print(f\"Shape of X after cleaning: {X.shape}\")\n",
    "\n",
    "# VÃ©rifiez les valeurs uniques de 'name' dans y\n",
    "print(\"Unique 'name' values in y:\", y['name'].nunique())\n",
    "\n",
    "# VÃ©rifiez les valeurs uniques de 'name' dans X\n",
    "print(\"Unique 'name' values in X:\", X['name'].nunique())\n",
    "\n",
    "# Noms dans y mais pas dans X\n",
    "missing_in_X = set(y['name']) - set(X['name'])\n",
    "print(f\"Names in y but not in X: {missing_in_X}\")\n",
    "\n",
    "# Noms dans X mais pas dans y\n",
    "missing_in_y = set(X['name']) - set(y['name'])\n",
    "print(f\"Names in X but not in y: {missing_in_y}\")\n",
    "\n",
    "# Trouver les noms communs\n",
    "common_names = set(y['name']).intersection(set(X['name']))\n",
    "\n",
    "# Filtrer y et X pour ne garder que les noms communs\n",
    "y = y[y['name'].isin(common_names)]\n",
    "X = X[X['name'].isin(common_names)]\n",
    "\n",
    "# VÃ©rifiez les dimensions aprÃ¨s nettoyage\n",
    "print(f\"Shape of y after alignment: {y.shape}\")\n",
    "print(f\"Shape of X after alignment: {X.shape}\")\n",
    "\n",
    "# Check for duplicates in 'name' in y\n",
    "print(\"Number of duplicate names in y:\", y['name'].duplicated().sum())\n",
    "\n",
    "# Check for duplicates in 'name' in X\n",
    "print(\"Number of duplicate names in X:\", X['name'].duplicated().sum())\n",
    "\n",
    "# Remove duplicates from both DataFrames\n",
    "y = y.drop_duplicates(subset=['name'])\n",
    "X = X.drop_duplicates(subset=['name'])\n",
    "\n",
    "# Verify the shapes again\n",
    "print(f\"Shape of y after removing duplicates: {y.shape}\")\n",
    "print(f\"Shape of X after removing duplicates: {X.shape}\")\n",
    "\n",
    "# Check if 'name' values are the same in both DataFrames\n",
    "common_names = set(y['name']) == set(X['name'])\n",
    "print(f\"Are 'name' values aligned between y and X {common_names}\")\n",
    "\n",
    "# Drop the 'name' column from X_volumes_clinical_with_outcome\n",
    "X_features = X.drop(columns=['name'])\n",
    "\n",
    "# imputation with median \n",
    "X_features_imputed = X_features.fillna(X_features.median())\n",
    "\n",
    "# Drop the 'name' column from y (if it exists)\n",
    "y_outcome = y.drop(columns=['name'])\n",
    "\n",
    "# Ensure y_outcome is a 1D array\n",
    "y_outcome = y_outcome.values.ravel()  # Convert to 1D if using pandas DataFrame\n",
    "\n",
    "FOLDS = 5\n",
    "N_REPEATS = 3\n",
    "nb_total_samples = len(y_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fcf9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41949b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5147e5e0-3e66-448b-b6c7-b520fd5a1ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoostingClassifier with hyperparameter gridsearch\n",
      "AUC (max): 0.93 +- 0.05\n",
      "F1 Score (max): 0.53 +- 0.1\n",
      "F2 Score (max): 0.62 +- 0.12\n",
      "Brier Score (min): 0.07 +- -0.02\n",
      "False negative: 2% +- 1\n",
      "False positive: 6% +- 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline_smote_under = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf = GridSearchCV(estimator=pipeline_smote_under, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv)\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling = cross_validate(clf, X_features_imputed, y_outcome, \n",
    "                                                scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, \n",
    "                                                cv=outer_cv, n_jobs=-1, return_estimator=True, return_indices=True)\n",
    "\n",
    "print(\"HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ad0b8",
   "metadata": {},
   "source": [
    "#### Confidence intervals with test set bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac1fda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_list = []\n",
    "\n",
    "for estimator in nested_scores_smote_undersampling[\"estimator\"]:\n",
    "    best_params_list.append(frozenset(estimator.best_params_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af383618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final chosen hyperparameters: {'model__learning_rate': 0.01, 'under__sampling_strategy': 0.7, 'over__sampling_strategy': 0.2, 'over__k_neighbors': 5}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Convert dict to a hashable tuple and count occurrences\n",
    "most_common_params = Counter(best_params_list).most_common(1)[0][0]\n",
    "final_params = dict(most_common_params)  # Convert back to dict\n",
    "\n",
    "\n",
    "print(\"Final chosen hyperparameters:\", final_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c18cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('over', SMOTE(k_neighbors=5, sampling_strategy=0.2)), ('under', RandomUnderSampler(sampling_strategy=0.7)), ('model', HistGradientBoostingClassifier(learning_rate=0.01))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92a390d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(pipeline, X_features_imputed, y_outcome, cv=20, method='predict_proba')[:,1]\n",
    "\n",
    "y_pred_binary = [1 if i>=0.5 else 0 for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "974f6686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, confusion_matrix, roc_auc_score, f1_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3098dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=12345)\n",
    "idx = np.arange(len(y_outcome))\n",
    "\n",
    "y_pred = np.asarray(y_pred)\n",
    "y_pred_binary = np.asarray(y_pred_binary)\n",
    "y = np.asarray(y_outcome)\n",
    "X_dropped = np.asarray(X_features_imputed)\n",
    "\n",
    "test_roc_auc = []\n",
    "test_f1 = []\n",
    "test_ftwos = []\n",
    "test_brier = []\n",
    "test_false_neg = []\n",
    "test_false_pos = []\n",
    "\n",
    "for i in range(200): # bootstrap with 200 rounds: random sampling with replacement of the predictions\n",
    "\n",
    "    pred_idx = rng.choice(idx, size=len(idx), replace=True)\n",
    "    \n",
    "    roc_auc_test_boot = roc_auc_score(y_score=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    f1_test_boot = f1_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx])\n",
    "    f2_test_boot = fbeta_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx], beta=2)\n",
    "    brier_test_boot = brier_score_loss(y_proba=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    false_neg_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[1,0]\n",
    "    false_pos_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[0,1]\n",
    "    \n",
    "    test_roc_auc.append(roc_auc_test_boot)\n",
    "    test_f1.append(f1_test_boot)\n",
    "    test_ftwos.append(f2_test_boot)\n",
    "    test_brier.append(brier_test_boot)\n",
    "    test_false_neg.append(false_neg_test_boot/len(idx)*100)\n",
    "    test_false_pos.append(false_pos_test_boot/len(idx)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "566b3caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification performance\n",
      "\n",
      "ROC AUC:         0.92;   95% CI 0.87-0.97\n",
      "F1:              0.55;   95% CI 0.42-0.66\n",
      "F2:              0.64;   95% CI 0.51-0.75\n",
      "Brier loss:      0.07;   95% CI 0.06-0.08\n",
      "False negatives: 1.67%;  95% CI 0.75-2.81\n",
      "False negatives: 5.40%; 95% CI 3.37-7.49\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification performance\\n\")\n",
    "\n",
    "bootstrap_roc_auc_test_mean = np.mean(test_roc_auc)\n",
    "ci_lower = np.percentile(test_roc_auc, 2.5)     # 2.5 percentile (alpha=0.025)\n",
    "ci_upper = np.percentile(test_roc_auc, 97.5)\n",
    "print(f\"ROC AUC:         {bootstrap_roc_auc_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f1_test_mean = np.mean(test_f1)\n",
    "ci_lower = np.percentile(test_f1, 2.5)\n",
    "ci_upper = np.percentile(test_f1, 97.5)\n",
    "print(f\"F1:              {bootstrap_f1_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f2_test_mean = np.mean(test_ftwos)\n",
    "ci_lower = np.percentile(test_ftwos, 2.5)\n",
    "ci_upper = np.percentile(test_ftwos, 97.5)\n",
    "print(f\"F2:              {bootstrap_f2_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_brier_test_mean = np.mean(test_brier)\n",
    "ci_lower = np.percentile(test_brier, 2.5)\n",
    "ci_upper = np.percentile(test_brier, 97.5)\n",
    "print(f\"Brier loss:      {bootstrap_brier_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_neg_test_mean = np.mean(test_false_neg)\n",
    "ci_lower = np.percentile(test_false_neg, 2.5)\n",
    "ci_upper = np.percentile(test_false_neg, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_neg_test_mean:.2f}%;  95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_pos_test_mean = np.mean(test_false_pos)\n",
    "ci_lower = np.percentile(test_false_pos, 2.5)\n",
    "ci_upper = np.percentile(test_false_pos, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_pos_test_mean:.2f}%; 95% CI {ci_lower:.2f}-{ci_upper:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797908a5",
   "metadata": {},
   "source": [
    "### Gradient boosting with prehospital + segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc7153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run preprocessing outcome mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "75324b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns 16 to 36\n",
    "X_prehosp_and_segmentation = data_full.iloc[:, 15:35]\n",
    "\n",
    "# Add segmentation columns\n",
    "X_prehosp_and_segmentation = pd.concat([X_prehosp_and_segmentation, data_full.iloc[:, 101:115]], axis=1)\n",
    "\n",
    "# Convert all columns to numeric, replacing non-numeric values and empty strings with NaN\n",
    "X_prehosp_and_segmentation = X_prehosp_and_segmentation.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Include column name\n",
    "X_prehosp_and_segmentation = pd.concat([X_prehosp_and_segmentation, data_full.iloc[:, 2]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "172f5ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values per column:\n",
      "PAS  SMUR                    40\n",
      "PAD  SMUR                    49\n",
      "FC SMUR                      42\n",
      "FR SMUR                     485\n",
      "Shock Index SMUR             55\n",
      "GCS SMUR                     17\n",
      "GCS (M) SMUR                 30\n",
      "HÃ©mocue SMUR                243\n",
      "Shock Index inversÃ©          54\n",
      "Shock index diastolique      62\n",
      "Anomalie pupille SMUR        17\n",
      "Fracas bassin                14\n",
      "Amputation                   12\n",
      "ACR SMUR                     11\n",
      "HÃ©morragie ext SMUR          12\n",
      "IschÃ©mie                     12\n",
      "Intubation prehosp           11\n",
      "Expansion volÃ©mique          20\n",
      "OsmoTH prehosp               11\n",
      "Vasopresseur prehosp         12\n",
      "supratentorial_IPH            0\n",
      "supratentorial_SAH            0\n",
      "supratentorial_Petechiae      0\n",
      "supratentorial_Edema          0\n",
      "infratentorial_IPH            0\n",
      "infratentorial_SAH            0\n",
      "infratentorial_Petechiae      0\n",
      "infratentorial_Edema          0\n",
      "brainstem_IPH                 0\n",
      "brainstem_SAH                 0\n",
      "brainstem_Petechiae           0\n",
      "brainstem_Edema               0\n",
      "SDH_y                         0\n",
      "EDH                           0\n",
      "name                          0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAS  SMUR</th>\n",
       "      <th>PAD  SMUR</th>\n",
       "      <th>FC SMUR</th>\n",
       "      <th>FR SMUR</th>\n",
       "      <th>Shock Index SMUR</th>\n",
       "      <th>GCS SMUR</th>\n",
       "      <th>GCS (M) SMUR</th>\n",
       "      <th>HÃ©mocue SMUR</th>\n",
       "      <th>Shock Index inversÃ©</th>\n",
       "      <th>Shock index diastolique</th>\n",
       "      <th>...</th>\n",
       "      <th>infratentorial_SAH</th>\n",
       "      <th>infratentorial_Petechiae</th>\n",
       "      <th>infratentorial_Edema</th>\n",
       "      <th>brainstem_IPH</th>\n",
       "      <th>brainstem_SAH</th>\n",
       "      <th>brainstem_Petechiae</th>\n",
       "      <th>brainstem_Edema</th>\n",
       "      <th>SDH_y</th>\n",
       "      <th>EDH</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.72</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2476</td>\n",
       "      <td>229</td>\n",
       "      <td>P0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.14</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>P0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>11685</td>\n",
       "      <td>P0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>P0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.51</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>796</td>\n",
       "      <td>0</td>\n",
       "      <td>P0004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PAS  SMUR   PAD  SMUR   FC SMUR   FR SMUR  Shock Index SMUR  GCS SMUR   \\\n",
       "0       190.0       103.0     137.0      NaN              0.72       15.0   \n",
       "1        87.0        49.0      56.0      NaN              0.64       15.0   \n",
       "2       100.0        60.0     100.0     17.0              1.00       15.0   \n",
       "3       101.0        64.0     120.0      NaN              1.19       14.0   \n",
       "4       110.0        71.0     107.0     18.0              0.97       15.0   \n",
       "\n",
       "   GCS (M) SMUR   HÃ©mocue SMUR   Shock Index inversÃ©  Shock index diastolique  \\\n",
       "0            6.0            NaN                 1.39                     1.33   \n",
       "1            6.0            NaN                 1.55                     1.14   \n",
       "2            6.0            NaN                 1.00                     1.67   \n",
       "3            6.0           13.1                 0.84                     1.88   \n",
       "4            6.0           15.8                 1.03                     1.51   \n",
       "\n",
       "   ...  infratentorial_SAH  infratentorial_Petechiae  infratentorial_Edema  \\\n",
       "0  ...                   0                         0                     0   \n",
       "1  ...                  15                         0                     0   \n",
       "2  ...                   0                         0                     0   \n",
       "3  ...                   0                         0                     0   \n",
       "4  ...                   0                         0                     0   \n",
       "\n",
       "   brainstem_IPH  brainstem_SAH  brainstem_Petechiae  brainstem_Edema  SDH_y  \\\n",
       "0              0              0                    0                0   2476   \n",
       "1              0              0                    0                0     43   \n",
       "2              0              0                    0                0    312   \n",
       "3              0              0                    0                0     11   \n",
       "4              0              0                    0                0    796   \n",
       "\n",
       "     EDH   name  \n",
       "0    229  P0000  \n",
       "1      0  P0001  \n",
       "2  11685  P0002  \n",
       "3      0  P0003  \n",
       "4      0  P0004  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Verify if there are NaN values\n",
    "print(f\"Number of NaN values per column:\\n{X_prehosp_and_segmentation.isna().sum()}\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "X_prehosp_and_segmentation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1ecfa3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name  mortality\n",
      "0  P0000          0\n",
      "1  P0001          0\n",
      "2  P0002          0\n",
      "3  P0003          0\n",
      "4  P0004          0\n",
      "Outcome events : 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bettik/PROJECTS/pr-gin5_aini/fehrdelt/FastDiag/data_preprocessing_outcomes.py:236: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col] = data[col].replace({'1': 1, '0': 0, 'nd': np.nan, '8 Upper Good Recovery (Upper GR)': np.nan}).astype(float)\n",
      "/bettik/PROJECTS/pr-gin5_aini/fehrdelt/FastDiag/data_preprocessing_outcomes.py:236: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col] = data[col].replace({'1': 1, '0': 0, 'nd': np.nan, '8 Upper Good Recovery (Upper GR)': np.nan}).astype(float)\n",
      "/bettik/PROJECTS/pr-gin5_aini/fehrdelt/FastDiag/data_preprocessing_outcomes.py:236: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col] = data[col].replace({'1': 1, '0': 0, 'nd': np.nan, '8 Upper Good Recovery (Upper GR)': np.nan}).astype(float)\n"
     ]
    }
   ],
   "source": [
    "X = X_prehosp_and_segmentation.copy()\n",
    "y = get_mortality_6m()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0b76bf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes with NaN in 'mortality': Index([], dtype='int64')\n",
      "Shape of y after cleaning: (566, 2)\n",
      "Shape of X after cleaning: (534, 35)\n",
      "Unique 'name' values in y: 562\n",
      "Unique 'name' values in X: 534\n",
      "Names in y but not in X: {'P0349', 'P0518', 'P0155', 'P0150', 'P0383', 'P0224', 'P0217', 'P0564', 'P0532', 'P0327', 'P0147', 'P0207', 'P0018', 'P0195', 'P0430', 'P0239', 'P0149', 'P0480', 'P0083', 'P0369', 'P0422', 'P0412', 'P0111', 'P0356', 'P0010', 'P0527', 'P0343', 'P0491'}\n",
      "Names in X but not in y: set()\n",
      "Shape of y after alignment: (536, 2)\n",
      "Shape of X after alignment: (534, 35)\n",
      "Number of duplicate names in y: 2\n",
      "Number of duplicate names in X: 0\n",
      "Shape of y after removing duplicates: (534, 2)\n",
      "Shape of X after removing duplicates: (534, 35)\n",
      "Are 'name' values aligned between y and X True\n"
     ]
    }
   ],
   "source": [
    "# Identify indexes where 'mortality' is NaN in y\n",
    "nan_indexes = y.loc[pd.isna(y[\"mortality\"]), :].index\n",
    "print(f\"Indexes with NaN in 'mortality': {nan_indexes}\")\n",
    "\n",
    "# Drop rows with NaN in y\n",
    "y = y.drop(index=nan_indexes)\n",
    "\n",
    "# Ensure 'name' is the index or used for alignment\n",
    "X = X[~X['name'].isin(y.loc[nan_indexes, 'name'])]\n",
    "\n",
    "# Verify the shapes after cleaning\n",
    "print(f\"Shape of y after cleaning: {y.shape}\")\n",
    "print(f\"Shape of X after cleaning: {X.shape}\")\n",
    "\n",
    "# VÃ©rifiez les valeurs uniques de 'name' dans y\n",
    "print(\"Unique 'name' values in y:\", y['name'].nunique())\n",
    "\n",
    "# VÃ©rifiez les valeurs uniques de 'name' dans X\n",
    "print(\"Unique 'name' values in X:\", X['name'].nunique())\n",
    "\n",
    "# Noms dans y mais pas dans X\n",
    "missing_in_X = set(y['name']) - set(X['name'])\n",
    "print(f\"Names in y but not in X: {missing_in_X}\")\n",
    "\n",
    "# Noms dans X mais pas dans y\n",
    "missing_in_y = set(X['name']) - set(y['name'])\n",
    "print(f\"Names in X but not in y: {missing_in_y}\")\n",
    "\n",
    "# Trouver les noms communs\n",
    "common_names = set(y['name']).intersection(set(X['name']))\n",
    "\n",
    "# Filtrer y et X pour ne garder que les noms communs\n",
    "y = y[y['name'].isin(common_names)]\n",
    "X = X[X['name'].isin(common_names)]\n",
    "\n",
    "# VÃ©rifiez les dimensions aprÃ¨s nettoyage\n",
    "print(f\"Shape of y after alignment: {y.shape}\")\n",
    "print(f\"Shape of X after alignment: {X.shape}\")\n",
    "\n",
    "# Check for duplicates in 'name' in y\n",
    "print(\"Number of duplicate names in y:\", y['name'].duplicated().sum())\n",
    "\n",
    "# Check for duplicates in 'name' in X\n",
    "print(\"Number of duplicate names in X:\", X['name'].duplicated().sum())\n",
    "\n",
    "# Remove duplicates from both DataFrames\n",
    "y = y.drop_duplicates(subset=['name'])\n",
    "X = X.drop_duplicates(subset=['name'])\n",
    "\n",
    "# Verify the shapes again\n",
    "print(f\"Shape of y after removing duplicates: {y.shape}\")\n",
    "print(f\"Shape of X after removing duplicates: {X.shape}\")\n",
    "\n",
    "# Check if 'name' values are the same in both DataFrames\n",
    "common_names = set(y['name']) == set(X['name'])\n",
    "print(f\"Are 'name' values aligned between y and X {common_names}\")\n",
    "\n",
    "# Drop the 'name' column from X_volumes_clinical_with_outcome\n",
    "X_features = X.drop(columns=['name'])\n",
    "\n",
    "# imputation with median \n",
    "X_features_imputed = X_features.fillna(X_features.median())\n",
    "\n",
    "# Drop the 'name' column from y (if it exists)\n",
    "y_outcome = y.drop(columns=['name'])\n",
    "\n",
    "# Ensure y_outcome is a 1D array\n",
    "y_outcome = y_outcome.values.ravel()  # Convert to 1D if using pandas DataFrame\n",
    "\n",
    "FOLDS = 5\n",
    "N_REPEATS = 3\n",
    "nb_total_samples = len(y_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c1b6d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8eb98e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoostingClassifier with hyperparameter gridsearch\n",
      "AUC (max): 0.94 +- 0.05\n",
      "F1 Score (max): 0.68 +- 0.11\n",
      "F2 Score (max): 0.72 +- 0.12\n",
      "Brier Score (min): 0.04 +- -0.01\n",
      "False negative: 2% +- 1\n",
      "False positive: 3% +- 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline_smote_under = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf = GridSearchCV(estimator=pipeline_smote_under, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv)\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling = cross_validate(clf, X_features_imputed, y_outcome, \n",
    "                                                scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, \n",
    "                                                cv=outer_cv, n_jobs=-1, return_estimator=True, return_indices=True)\n",
    "\n",
    "print(\"HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e5dce",
   "metadata": {},
   "source": [
    "#### Confidence intervals with test set bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c858796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_list = []\n",
    "\n",
    "for estimator in nested_scores_smote_undersampling[\"estimator\"]:\n",
    "    best_params_list.append(frozenset(estimator.best_params_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "025a0bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final chosen hyperparameters: {'model__learning_rate': 0.2, 'under__sampling_strategy': 0.7, 'over__k_neighbors': 8, 'over__sampling_strategy': 0.3}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Convert dict to a hashable tuple and count occurrences\n",
    "most_common_params = Counter(best_params_list).most_common(1)[0][0]\n",
    "final_params = dict(most_common_params)  # Convert back to dict\n",
    "\n",
    "\n",
    "print(\"Final chosen hyperparameters:\", final_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "537255a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('over', SMOTE(k_neighbors=8, sampling_strategy=0.3)), ('under', RandomUnderSampler(sampling_strategy=0.7)), ('model', HistGradientBoostingClassifier(learning_rate=0.2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "545b698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(pipeline, X_features_imputed, y_outcome, cv=20, method='predict_proba')[:,1]\n",
    "\n",
    "y_pred_binary = [1 if i>=0.5 else 0 for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dc6c94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, confusion_matrix, roc_auc_score, f1_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3e72b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=12345)\n",
    "idx = np.arange(len(y_outcome))\n",
    "\n",
    "y_pred = np.asarray(y_pred)\n",
    "y_pred_binary = np.asarray(y_pred_binary)\n",
    "y = np.asarray(y_outcome)\n",
    "X_dropped = np.asarray(X_features_imputed)\n",
    "\n",
    "test_roc_auc = []\n",
    "test_f1 = []\n",
    "test_ftwos = []\n",
    "test_brier = []\n",
    "test_false_neg = []\n",
    "test_false_pos = []\n",
    "\n",
    "for i in range(200): # bootstrap with 200 rounds: random sampling with replacement of the predictions\n",
    "\n",
    "    pred_idx = rng.choice(idx, size=len(idx), replace=True)\n",
    "    \n",
    "    roc_auc_test_boot = roc_auc_score(y_score=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    f1_test_boot = f1_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx])\n",
    "    f2_test_boot = fbeta_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx], beta=2)\n",
    "    brier_test_boot = brier_score_loss(y_proba=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    false_neg_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[1,0]\n",
    "    false_pos_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[0,1]\n",
    "    \n",
    "    test_roc_auc.append(roc_auc_test_boot)\n",
    "    test_f1.append(f1_test_boot)\n",
    "    test_ftwos.append(f2_test_boot)\n",
    "    test_brier.append(brier_test_boot)\n",
    "    test_false_neg.append(false_neg_test_boot/len(idx)*100)\n",
    "    test_false_pos.append(false_pos_test_boot/len(idx)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "16013edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification performance\n",
      "\n",
      "ROC AUC:         0.94;   95% CI 0.87-0.99\n",
      "F1:              0.67;   95% CI 0.55-0.78\n",
      "F2:              0.70;   95% CI 0.57-0.82\n",
      "Brier loss:      0.04;   95% CI 0.02-0.05\n",
      "False negatives: 1.65%;  95% CI 0.75-2.81\n",
      "False negatives: 2.67%; 95% CI 1.31-3.94\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification performance\\n\")\n",
    "\n",
    "bootstrap_roc_auc_test_mean = np.mean(test_roc_auc)\n",
    "ci_lower = np.percentile(test_roc_auc, 2.5)     # 2.5 percentile (alpha=0.025)\n",
    "ci_upper = np.percentile(test_roc_auc, 97.5)\n",
    "print(f\"ROC AUC:         {bootstrap_roc_auc_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f1_test_mean = np.mean(test_f1)\n",
    "ci_lower = np.percentile(test_f1, 2.5)\n",
    "ci_upper = np.percentile(test_f1, 97.5)\n",
    "print(f\"F1:              {bootstrap_f1_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f2_test_mean = np.mean(test_ftwos)\n",
    "ci_lower = np.percentile(test_ftwos, 2.5)\n",
    "ci_upper = np.percentile(test_ftwos, 97.5)\n",
    "print(f\"F2:              {bootstrap_f2_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_brier_test_mean = np.mean(test_brier)\n",
    "ci_lower = np.percentile(test_brier, 2.5)\n",
    "ci_upper = np.percentile(test_brier, 97.5)\n",
    "print(f\"Brier loss:      {bootstrap_brier_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_neg_test_mean = np.mean(test_false_neg)\n",
    "ci_lower = np.percentile(test_false_neg, 2.5)\n",
    "ci_upper = np.percentile(test_false_neg, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_neg_test_mean:.2f}%;  95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_pos_test_mean = np.mean(test_false_pos)\n",
    "ci_lower = np.percentile(test_false_pos, 2.5)\n",
    "ci_upper = np.percentile(test_false_pos, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_pos_test_mean:.2f}%; 95% CI {ci_lower:.2f}-{ci_upper:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabbe4b-1a9b-48a8-822a-6630894899e1",
   "metadata": {},
   "source": [
    "### Gradient boosting with all DCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab607165-596f-4a88-840c-425dcd331ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run preproc outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f881797-c414-45a7-b86d-e94c2b413422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAS DCA</th>\n",
       "      <th>PAD DCA</th>\n",
       "      <th>FC  DCA</th>\n",
       "      <th>Shock index DCA</th>\n",
       "      <th>Shock Index inversÃ©.1</th>\n",
       "      <th>Shock index diastolique.1</th>\n",
       "      <th>GCS DCA</th>\n",
       "      <th>GCS (M) DCA</th>\n",
       "      <th>TempÃ©rature DCA</th>\n",
       "      <th>HÃ©mocue DCA</th>\n",
       "      <th>Dextro DCA (mmol/l)</th>\n",
       "      <th>DTC Vd</th>\n",
       "      <th>DTC IP</th>\n",
       "      <th>OsmothÃ©rapie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>87</td>\n",
       "      <td>132</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.52</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>14.3</td>\n",
       "      <td>nd</td>\n",
       "      <td>nd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>35.2</td>\n",
       "      <td>14</td>\n",
       "      <td>5.9</td>\n",
       "      <td>46</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>nd</td>\n",
       "      <td>38.2</td>\n",
       "      <td>12.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>55</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.33</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>6.8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.83</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>9.4</td>\n",
       "      <td>nd</td>\n",
       "      <td>nd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PAS DCA PAD DCA FC  DCA Shock index DCA Shock Index inversÃ©.1  \\\n",
       "0     120      87     132             1.1                  0.91   \n",
       "1     110      60      60            0.55                  1.83   \n",
       "2     110      50     100            0.91                   1.1   \n",
       "3     120      90     120               1                     1   \n",
       "4     120      60     110            0.92                  1.09   \n",
       "\n",
       "  Shock index diastolique.1 GCS DCA GCS (M) DCA TempÃ©rature DCA HÃ©mocue DCA  \\\n",
       "0                      1.52      15           6            37.2        13.7   \n",
       "1                         1      15           6            35.2          14   \n",
       "2                         2      13          nd            38.2        12.7   \n",
       "3                      1.33      15           6            37.8        12.8   \n",
       "4                      1.83      15           6            37.1        15.6   \n",
       "\n",
       "  Dextro DCA (mmol/l) DTC Vd DTC IP OsmothÃ©rapie  \n",
       "0                14.3     nd     nd            0  \n",
       "1                 5.9     46    0.8            0  \n",
       "2                 6.4     55    1.3            0  \n",
       "3                 6.8     32    0.9            0  \n",
       "4                 9.4     nd     nd            0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select columns\n",
    "X_dca = data_full.iloc[:, 35:49]\n",
    "\n",
    "# Convert all columns to numeric, replacing non-numeric values and empty strings with NaN\n",
    "#X_dca = X_dca.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "X_dca.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6045136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values per column:\n",
      "PAS DCA                       12\n",
      "PAD DCA                       12\n",
      "FC  DCA                       13\n",
      "Shock index DCA                0\n",
      "Shock Index inversÃ©.1          0\n",
      "Shock index diastolique.1      0\n",
      "GCS DCA                       14\n",
      "GCS (M) DCA                   20\n",
      "TempÃ©rature DCA               20\n",
      "HÃ©mocue DCA                   25\n",
      "Dextro DCA (mmol/l)           53\n",
      "DTC Vd                        16\n",
      "DTC IP                       329\n",
      "OsmothÃ©rapie                  11\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAS DCA</th>\n",
       "      <th>PAD DCA</th>\n",
       "      <th>FC  DCA</th>\n",
       "      <th>Shock index DCA</th>\n",
       "      <th>Shock Index inversÃ©.1</th>\n",
       "      <th>Shock index diastolique.1</th>\n",
       "      <th>GCS DCA</th>\n",
       "      <th>GCS (M) DCA</th>\n",
       "      <th>TempÃ©rature DCA</th>\n",
       "      <th>HÃ©mocue DCA</th>\n",
       "      <th>Dextro DCA (mmol/l)</th>\n",
       "      <th>DTC Vd</th>\n",
       "      <th>DTC IP</th>\n",
       "      <th>OsmothÃ©rapie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>87</td>\n",
       "      <td>132</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.52</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>14.3</td>\n",
       "      <td>nd</td>\n",
       "      <td>nd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>35.2</td>\n",
       "      <td>14</td>\n",
       "      <td>5.9</td>\n",
       "      <td>46</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>nd</td>\n",
       "      <td>38.2</td>\n",
       "      <td>12.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>55</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.33</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>6.8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.83</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>9.4</td>\n",
       "      <td>nd</td>\n",
       "      <td>nd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PAS DCA PAD DCA FC  DCA Shock index DCA Shock Index inversÃ©.1  \\\n",
       "0     120      87     132             1.1                  0.91   \n",
       "1     110      60      60            0.55                  1.83   \n",
       "2     110      50     100            0.91                   1.1   \n",
       "3     120      90     120               1                     1   \n",
       "4     120      60     110            0.92                  1.09   \n",
       "\n",
       "  Shock index diastolique.1 GCS DCA GCS (M) DCA TempÃ©rature DCA HÃ©mocue DCA  \\\n",
       "0                      1.52      15           6            37.2        13.7   \n",
       "1                         1      15           6            35.2          14   \n",
       "2                         2      13          nd            38.2        12.7   \n",
       "3                      1.33      15           6            37.8        12.8   \n",
       "4                      1.83      15           6            37.1        15.6   \n",
       "\n",
       "  Dextro DCA (mmol/l) DTC Vd DTC IP OsmothÃ©rapie  \n",
       "0                14.3     nd     nd            0  \n",
       "1                 5.9     46    0.8            0  \n",
       "2                 6.4     55    1.3            0  \n",
       "3                 6.8     32    0.9            0  \n",
       "4                 9.4     nd     nd            0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Verify if there are NaN values\n",
    "print(f\"Number of NaN values per column:\\n{X_dca.isna().sum()}\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "X_dca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a576b044-2342-47a3-a06b-04c56ce4b257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAS DCA</th>\n",
       "      <th>PAD DCA</th>\n",
       "      <th>FC  DCA</th>\n",
       "      <th>Shock index DCA</th>\n",
       "      <th>Shock Index inversÃ©.1</th>\n",
       "      <th>Shock index diastolique.1</th>\n",
       "      <th>GCS DCA</th>\n",
       "      <th>GCS (M) DCA</th>\n",
       "      <th>TempÃ©rature DCA</th>\n",
       "      <th>HÃ©mocue DCA</th>\n",
       "      <th>Dextro DCA (mmol/l)</th>\n",
       "      <th>DTC Vd</th>\n",
       "      <th>DTC IP</th>\n",
       "      <th>OsmothÃ©rapie</th>\n",
       "      <th>dtc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>87</td>\n",
       "      <td>132</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.52</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>14.3</td>\n",
       "      <td>nd</td>\n",
       "      <td>nd</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>35.2</td>\n",
       "      <td>14</td>\n",
       "      <td>5.9</td>\n",
       "      <td>46</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>nd</td>\n",
       "      <td>38.2</td>\n",
       "      <td>12.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>55</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.33</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>6.8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.83</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>9.4</td>\n",
       "      <td>nd</td>\n",
       "      <td>nd</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PAS DCA PAD DCA FC  DCA Shock index DCA Shock Index inversÃ©.1  \\\n",
       "0     120      87     132             1.1                  0.91   \n",
       "1     110      60      60            0.55                  1.83   \n",
       "2     110      50     100            0.91                   1.1   \n",
       "3     120      90     120               1                     1   \n",
       "4     120      60     110            0.92                  1.09   \n",
       "\n",
       "  Shock index diastolique.1 GCS DCA GCS (M) DCA TempÃ©rature DCA HÃ©mocue DCA  \\\n",
       "0                      1.52      15           6            37.2        13.7   \n",
       "1                         1      15           6            35.2          14   \n",
       "2                         2      13          nd            38.2        12.7   \n",
       "3                      1.33      15           6            37.8        12.8   \n",
       "4                      1.83      15           6            37.1        15.6   \n",
       "\n",
       "  Dextro DCA (mmol/l) DTC Vd DTC IP OsmothÃ©rapie  dtc  \n",
       "0                14.3     nd     nd            0  0.0  \n",
       "1                 5.9     46    0.8            0  0.0  \n",
       "2                 6.4     55    1.3            0  1.0  \n",
       "3                 6.8     32    0.9            0  0.0  \n",
       "4                 9.4     nd     nd            0  0.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the conditions for dtc\n",
    "def calculate_dtc(row):\n",
    "    # Condition for dtc = NA\n",
    "    if ('nr' in [row['DTC Vd'], row['DTC IP']] or\n",
    "        'non rÃ©alisÃ©' in [row['DTC Vd'], row['DTC IP']]):\n",
    "        return np.nan\n",
    "    # Condition for dtc = 1\n",
    "    if (row['DTC Vd'] == 'Pathologique' or row['DTC IP'] == 'Pathologique' or\n",
    "        (is_numeric(row['DTC Vd']) and float(row['DTC Vd']) < 30) or\n",
    "        (is_numeric(row['DTC IP']) and float(row['DTC IP']) > 1.2)):\n",
    "        return 1\n",
    "    # Default case for dtc = 0\n",
    "    return 0\n",
    "\n",
    "# Helper function to check if a value is numeric\n",
    "def is_numeric(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "# Apply the function row-wise to create the new column\n",
    "X_dca['dtc'] = X_dca.apply(calculate_dtc, axis=1)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "X_dca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7766877-9380-439e-b510-2fb0c92235a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values per column:\n",
      "PAS DCA                       14\n",
      "PAD DCA                       14\n",
      "FC  DCA                       17\n",
      "Shock index DCA               20\n",
      "Shock Index inversÃ©.1         20\n",
      "Shock index diastolique.1     20\n",
      "GCS DCA                       30\n",
      "GCS (M) DCA                   33\n",
      "TempÃ©rature DCA              402\n",
      "HÃ©mocue DCA                   37\n",
      "Dextro DCA (mmol/l)           75\n",
      "DTC Vd                       453\n",
      "DTC IP                       452\n",
      "OsmothÃ©rapie                 395\n",
      "dtc                          163\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert all columns to numeric, replacing non-numeric values and empty strings with NaN\n",
    "X_dca = X_dca.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "X_dca.head()\n",
    "\n",
    "# Verify if there are NaN values\n",
    "print(f\"Number of NaN values per column:\\n{X_dca.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a36e772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name  mortality\n",
      "0  P0000          0\n",
      "1  P0001          0\n",
      "2  P0002          0\n",
      "3  P0003          0\n",
      "4  P0004          0\n",
      "Outcome events : 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bettik/PROJECTS/pr-gin5_aini/fehrdelt/FastDiag/data_preprocessing_outcomes.py:236: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col] = data[col].replace({'1': 1, '0': 0, 'nd': np.nan, '8 Upper Good Recovery (Upper GR)': np.nan}).astype(float)\n",
      "/bettik/PROJECTS/pr-gin5_aini/fehrdelt/FastDiag/data_preprocessing_outcomes.py:236: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col] = data[col].replace({'1': 1, '0': 0, 'nd': np.nan, '8 Upper Good Recovery (Upper GR)': np.nan}).astype(float)\n",
      "/bettik/PROJECTS/pr-gin5_aini/fehrdelt/FastDiag/data_preprocessing_outcomes.py:236: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col] = data[col].replace({'1': 1, '0': 0, 'nd': np.nan, '8 Upper Good Recovery (Upper GR)': np.nan}).astype(float)\n"
     ]
    }
   ],
   "source": [
    "# Include column name\n",
    "X_dca = pd.concat([X_dca, data_full.iloc[:, 2]], axis=1)\n",
    "X = X_dca.copy()\n",
    "y = get_mortality_6m()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da6d88fc-8c5a-44fb-b5a2-239477df9b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes with NaN in 'mortality': Index([], dtype='int64')\n",
      "Shape of y after cleaning: (534, 2)\n",
      "Shape of X after cleaning: (534, 16)\n",
      "Unique 'name' values in y: 534\n",
      "Unique 'name' values in X: 534\n",
      "Names in y but not in X: set()\n",
      "Names in X but not in y: set()\n",
      "Shape of y after alignment: (534, 2)\n",
      "Shape of X after alignment: (534, 16)\n",
      "Number of duplicate names in y: 0\n",
      "Number of duplicate names in X: 0\n",
      "Shape of y after removing duplicates: (534, 2)\n",
      "Shape of X after removing duplicates: (534, 16)\n",
      "Are 'name' values aligned between y and X True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Identify indexes where 'mortality' is NaN in y\n",
    "nan_indexes = y.loc[pd.isna(y[\"mortality\"]), :].index\n",
    "print(f\"Indexes with NaN in 'mortality': {nan_indexes}\")\n",
    "\n",
    "# Drop rows with NaN in y\n",
    "y = y.drop(index=nan_indexes)\n",
    "\n",
    "# Ensure 'name' is the index or used for alignment\n",
    "X = X[~X['name'].isin(y.loc[nan_indexes, 'name'])]\n",
    "\n",
    "# Verify the shapes after cleaning\n",
    "print(f\"Shape of y after cleaning: {y.shape}\")\n",
    "print(f\"Shape of X after cleaning: {X.shape}\")\n",
    "\n",
    "# VÃ©rifiez les valeurs uniques de 'name' dans y\n",
    "print(\"Unique 'name' values in y:\", y['name'].nunique())\n",
    "\n",
    "# VÃ©rifiez les valeurs uniques de 'name' dans X\n",
    "print(\"Unique 'name' values in X:\", X['name'].nunique())\n",
    "\n",
    "# Noms dans y mais pas dans X\n",
    "missing_in_X = set(y['name']) - set(X['name'])\n",
    "print(f\"Names in y but not in X: {missing_in_X}\")\n",
    "\n",
    "# Noms dans X mais pas dans y\n",
    "missing_in_y = set(X['name']) - set(y['name'])\n",
    "print(f\"Names in X but not in y: {missing_in_y}\")\n",
    "\n",
    "# Trouver les noms communs\n",
    "common_names = set(y['name']).intersection(set(X['name']))\n",
    "\n",
    "# Filtrer y et X pour ne garder que les noms communs\n",
    "y = y[y['name'].isin(common_names)]\n",
    "X = X[X['name'].isin(common_names)]\n",
    "\n",
    "# VÃ©rifiez les dimensions aprÃ¨s nettoyage\n",
    "print(f\"Shape of y after alignment: {y.shape}\")\n",
    "print(f\"Shape of X after alignment: {X.shape}\")\n",
    "\n",
    "# Check for duplicates in 'name' in y\n",
    "print(\"Number of duplicate names in y:\", y['name'].duplicated().sum())\n",
    "\n",
    "# Check for duplicates in 'name' in X\n",
    "print(\"Number of duplicate names in X:\", X['name'].duplicated().sum())\n",
    "\n",
    "# Remove duplicates from both DataFrames\n",
    "y = y.drop_duplicates(subset=['name'])\n",
    "X = X.drop_duplicates(subset=['name'])\n",
    "\n",
    "# Verify the shapes again\n",
    "print(f\"Shape of y after removing duplicates: {y.shape}\")\n",
    "print(f\"Shape of X after removing duplicates: {X.shape}\")\n",
    "\n",
    "# Check if 'name' values are the same in both DataFrames\n",
    "common_names = set(y['name']) == set(X['name'])\n",
    "print(f\"Are 'name' values aligned between y and X {common_names}\")\n",
    "\n",
    "# Trier y et X par la colonne 'name'\n",
    "y = y.sort_values(by='name').reset_index(drop=True)\n",
    "X = X.sort_values(by='name').reset_index(drop=True)\n",
    "\n",
    "# VÃ©rifier si les noms sont alignÃ©s\n",
    "print((y['name'].values == X['name'].values).all())\n",
    "\n",
    "# Drop the 'name' column from X\n",
    "#X_features = X.drop(columns=['name', 'mortalitÃ© J7'])\n",
    "X_features = X.drop(columns=['name'])\n",
    "\n",
    "# imputation with median \n",
    "X_features_imputed = X_features.fillna(X_features.median())\n",
    "\n",
    "# Drop the 'name' column from y (if it exists)\n",
    "y_outcome = y.drop(columns=['name'])\n",
    "\n",
    "# Ensure y_outcome is a 1D array\n",
    "y_outcome = y_outcome.values.ravel()  # Convert to 1D if using pandas DataFrame\n",
    "\n",
    "FOLDS = 5\n",
    "N_REPEATS = 3\n",
    "nb_total_samples = len(y_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac10ad2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAS DCA</th>\n",
       "      <th>PAD DCA</th>\n",
       "      <th>FC  DCA</th>\n",
       "      <th>Shock index DCA</th>\n",
       "      <th>Shock Index inversÃ©.1</th>\n",
       "      <th>Shock index diastolique.1</th>\n",
       "      <th>GCS DCA</th>\n",
       "      <th>GCS (M) DCA</th>\n",
       "      <th>TempÃ©rature DCA</th>\n",
       "      <th>HÃ©mocue DCA</th>\n",
       "      <th>Dextro DCA (mmol/l)</th>\n",
       "      <th>DTC Vd</th>\n",
       "      <th>DTC IP</th>\n",
       "      <th>OsmothÃ©rapie</th>\n",
       "      <th>dtc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.52</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>14.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>35.2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.2</td>\n",
       "      <td>12.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.33</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>37.8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>6.8</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.83</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>37.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>9.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PAS DCA  PAD DCA  FC  DCA  Shock index DCA  Shock Index inversÃ©.1  \\\n",
       "0    120.0     87.0    132.0             1.10                   0.91   \n",
       "1    110.0     60.0     60.0             0.55                   1.83   \n",
       "2    110.0     50.0    100.0             0.91                   1.10   \n",
       "3    120.0     90.0    120.0             1.00                   1.00   \n",
       "4    120.0     60.0    110.0             0.92                   1.09   \n",
       "\n",
       "   Shock index diastolique.1  GCS DCA  GCS (M) DCA  TempÃ©rature DCA  \\\n",
       "0                       1.52     15.0          6.0             37.2   \n",
       "1                       1.00     15.0          6.0             35.2   \n",
       "2                       2.00     13.0          NaN             38.2   \n",
       "3                       1.33     15.0          6.0             37.8   \n",
       "4                       1.83     15.0          6.0             37.1   \n",
       "\n",
       "   HÃ©mocue DCA  Dextro DCA (mmol/l)  DTC Vd  DTC IP  OsmothÃ©rapie  dtc  \n",
       "0         13.7                 14.3     NaN     NaN           0.0  0.0  \n",
       "1         14.0                  5.9    46.0     0.8           0.0  0.0  \n",
       "2         12.7                  6.4    55.0     1.3           0.0  1.0  \n",
       "3         12.8                  6.8    32.0     0.9           0.0  0.0  \n",
       "4         15.6                  9.4     NaN     NaN           0.0  0.0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4848be20-adbc-40dd-9140-988736609fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoostingClassifier with hyperparameter gridsearch\n",
      "AUC (max): 0.88 +- 0.07\n",
      "F1 Score (max): 0.45 +- 0.1\n",
      "F2 Score (max): 0.56 +- 0.12\n",
      "Brier Score (min): 0.09 +- -0.01\n",
      "False negative: 2% +- 1\n",
      "False positive: 9% +- 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline_smote_under = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf = GridSearchCV(estimator=pipeline_smote_under, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv)\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling = cross_validate(clf, X_features_imputed, y_outcome, \n",
    "                                                scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, \n",
    "                                                cv=outer_cv, n_jobs=-1, return_estimator=True, return_indices=True)\n",
    "\n",
    "print(\"HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700751fc",
   "metadata": {},
   "source": [
    "#### Confidence intervals with test set bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9dabd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_list = []\n",
    "\n",
    "for estimator in nested_scores_smote_undersampling[\"estimator\"]:\n",
    "    best_params_list.append(frozenset(estimator.best_params_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f75d3f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final chosen hyperparameters: {'model__learning_rate': 0.01, 'under__sampling_strategy': 0.7, 'over__k_neighbors': 3, 'over__sampling_strategy': 0.2}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Convert dict to a hashable tuple and count occurrences\n",
    "most_common_params = Counter(best_params_list).most_common(1)[0][0]\n",
    "final_params = dict(most_common_params)  # Convert back to dict\n",
    "\n",
    "\n",
    "print(\"Final chosen hyperparameters:\", final_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eaa41af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('over', SMOTE(k_neighbors=3, sampling_strategy=0.2)), ('under', RandomUnderSampler(sampling_strategy=0.7)), ('model', HistGradientBoostingClassifier(learning_rate=0.01))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8fd63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(pipeline, X_features_imputed, y_outcome, cv=20, method='predict_proba')[:,1]\n",
    "\n",
    "y_pred_binary = [1 if i>=0.5 else 0 for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8687d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, confusion_matrix, roc_auc_score, f1_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e3cfa309",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=12345)\n",
    "idx = np.arange(len(y_outcome))\n",
    "\n",
    "y_pred = np.asarray(y_pred)\n",
    "y_pred_binary = np.asarray(y_pred_binary)\n",
    "y = np.asarray(y_outcome)\n",
    "X_dropped = np.asarray(X_features_imputed)\n",
    "\n",
    "test_roc_auc = []\n",
    "test_f1 = []\n",
    "test_ftwos = []\n",
    "test_brier = []\n",
    "test_false_neg = []\n",
    "test_false_pos = []\n",
    "\n",
    "for i in range(200): # bootstrap with 200 rounds: random sampling with replacement of the predictions\n",
    "\n",
    "    pred_idx = rng.choice(idx, size=len(idx), replace=True)\n",
    "    \n",
    "    roc_auc_test_boot = roc_auc_score(y_score=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    f1_test_boot = f1_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx])\n",
    "    f2_test_boot = fbeta_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx], beta=2)\n",
    "    brier_test_boot = brier_score_loss(y_proba=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    false_neg_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[1,0]\n",
    "    false_pos_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[0,1]\n",
    "    \n",
    "    test_roc_auc.append(roc_auc_test_boot)\n",
    "    test_f1.append(f1_test_boot)\n",
    "    test_ftwos.append(f2_test_boot)\n",
    "    test_brier.append(brier_test_boot)\n",
    "    test_false_neg.append(false_neg_test_boot/len(idx)*100)\n",
    "    test_false_pos.append(false_pos_test_boot/len(idx)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d97752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification performance\n",
      "\n",
      "ROC AUC:         0.87;   95% CI 0.81-0.94\n",
      "F1:              0.46;   95% CI 0.33-0.57\n",
      "F2:              0.58;   95% CI 0.44-0.69\n",
      "Brier loss:      0.09;   95% CI 0.08-0.10\n",
      "False negatives: 1.84%;  95% CI 0.74-3.18\n",
      "False negatives: 8.16%; 95% CI 5.99-10.49\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification performance\\n\")\n",
    "\n",
    "bootstrap_roc_auc_test_mean = np.mean(test_roc_auc)\n",
    "ci_lower = np.percentile(test_roc_auc, 2.5)     # 2.5 percentile (alpha=0.025)\n",
    "ci_upper = np.percentile(test_roc_auc, 97.5)\n",
    "print(f\"ROC AUC:         {bootstrap_roc_auc_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f1_test_mean = np.mean(test_f1)\n",
    "ci_lower = np.percentile(test_f1, 2.5)\n",
    "ci_upper = np.percentile(test_f1, 97.5)\n",
    "print(f\"F1:              {bootstrap_f1_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f2_test_mean = np.mean(test_ftwos)\n",
    "ci_lower = np.percentile(test_ftwos, 2.5)\n",
    "ci_upper = np.percentile(test_ftwos, 97.5)\n",
    "print(f\"F2:              {bootstrap_f2_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_brier_test_mean = np.mean(test_brier)\n",
    "ci_lower = np.percentile(test_brier, 2.5)\n",
    "ci_upper = np.percentile(test_brier, 97.5)\n",
    "print(f\"Brier loss:      {bootstrap_brier_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_neg_test_mean = np.mean(test_false_neg)\n",
    "ci_lower = np.percentile(test_false_neg, 2.5)\n",
    "ci_upper = np.percentile(test_false_neg, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_neg_test_mean:.2f}%;  95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_pos_test_mean = np.mean(test_false_pos)\n",
    "ci_lower = np.percentile(test_false_pos, 2.5)\n",
    "ci_upper = np.percentile(test_false_pos, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_pos_test_mean:.2f}%; 95% CI {ci_lower:.2f}-{ci_upper:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40882def",
   "metadata": {},
   "source": [
    "### Gradient boosting with all DCA + segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "547ad95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run preproc outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f0322828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAS DCA</th>\n",
       "      <th>PAD DCA</th>\n",
       "      <th>FC  DCA</th>\n",
       "      <th>Shock index DCA</th>\n",
       "      <th>Shock Index inversÃ©.1</th>\n",
       "      <th>Shock index diastolique.1</th>\n",
       "      <th>GCS DCA</th>\n",
       "      <th>GCS (M) DCA</th>\n",
       "      <th>TempÃ©rature DCA</th>\n",
       "      <th>HÃ©mocue DCA</th>\n",
       "      <th>...</th>\n",
       "      <th>infratentorial_IPH</th>\n",
       "      <th>infratentorial_SAH</th>\n",
       "      <th>infratentorial_Petechiae</th>\n",
       "      <th>infratentorial_Edema</th>\n",
       "      <th>brainstem_IPH</th>\n",
       "      <th>brainstem_SAH</th>\n",
       "      <th>brainstem_Petechiae</th>\n",
       "      <th>brainstem_Edema</th>\n",
       "      <th>SDH_y</th>\n",
       "      <th>EDH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>87</td>\n",
       "      <td>132</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.52</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2476</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>35.2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>nd</td>\n",
       "      <td>38.2</td>\n",
       "      <td>12.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>11685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.33</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.83</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>796</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PAS DCA PAD DCA FC  DCA Shock index DCA Shock Index inversÃ©.1  \\\n",
       "0     120      87     132             1.1                  0.91   \n",
       "1     110      60      60            0.55                  1.83   \n",
       "2     110      50     100            0.91                   1.1   \n",
       "3     120      90     120               1                     1   \n",
       "4     120      60     110            0.92                  1.09   \n",
       "\n",
       "  Shock index diastolique.1 GCS DCA GCS (M) DCA TempÃ©rature DCA HÃ©mocue DCA  \\\n",
       "0                      1.52      15           6            37.2        13.7   \n",
       "1                         1      15           6            35.2          14   \n",
       "2                         2      13          nd            38.2        12.7   \n",
       "3                      1.33      15           6            37.8        12.8   \n",
       "4                      1.83      15           6            37.1        15.6   \n",
       "\n",
       "   ... infratentorial_IPH infratentorial_SAH infratentorial_Petechiae  \\\n",
       "0  ...                  0                  0                        0   \n",
       "1  ...                  0                 15                        0   \n",
       "2  ...                  0                  0                        0   \n",
       "3  ...                  0                  0                        0   \n",
       "4  ...                  0                  0                        0   \n",
       "\n",
       "  infratentorial_Edema  brainstem_IPH  brainstem_SAH  brainstem_Petechiae  \\\n",
       "0                    0              0              0                    0   \n",
       "1                    0              0              0                    0   \n",
       "2                    0              0              0                    0   \n",
       "3                    0              0              0                    0   \n",
       "4                    0              0              0                    0   \n",
       "\n",
       "   brainstem_Edema  SDH_y    EDH  \n",
       "0                0   2476    229  \n",
       "1                0     43      0  \n",
       "2                0    312  11685  \n",
       "3                0     11      0  \n",
       "4                0    796      0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select columns (DCA)\n",
    "X_dca_and_segmentation = data_full.iloc[:, 35:49] # add 92:\n",
    "\n",
    "# Add segmentation columns\n",
    "X_dca_and_segmentation = pd.concat([X_dca_and_segmentation, data_full.iloc[:, 101:115]], axis=1)\n",
    "\n",
    "\n",
    "# Convert all columns to numeric, replacing non-numeric values and empty strings with NaN\n",
    "#X_dca = X_dca.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "X_dca_and_segmentation.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a54e46cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values per column:\n",
      "PAS DCA                       12\n",
      "PAD DCA                       12\n",
      "FC  DCA                       13\n",
      "Shock index DCA                0\n",
      "Shock Index inversÃ©.1          0\n",
      "Shock index diastolique.1      0\n",
      "GCS DCA                       14\n",
      "GCS (M) DCA                   20\n",
      "TempÃ©rature DCA               20\n",
      "HÃ©mocue DCA                   25\n",
      "Dextro DCA (mmol/l)           53\n",
      "DTC Vd                        16\n",
      "DTC IP                       329\n",
      "OsmothÃ©rapie                  11\n",
      "supratentorial_IPH             0\n",
      "supratentorial_SAH             0\n",
      "supratentorial_Petechiae       0\n",
      "supratentorial_Edema           0\n",
      "infratentorial_IPH             0\n",
      "infratentorial_SAH             0\n",
      "infratentorial_Petechiae       0\n",
      "infratentorial_Edema           0\n",
      "brainstem_IPH                  0\n",
      "brainstem_SAH                  0\n",
      "brainstem_Petechiae            0\n",
      "brainstem_Edema                0\n",
      "SDH_y                          0\n",
      "EDH                            0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAS DCA</th>\n",
       "      <th>PAD DCA</th>\n",
       "      <th>FC  DCA</th>\n",
       "      <th>Shock index DCA</th>\n",
       "      <th>Shock Index inversÃ©.1</th>\n",
       "      <th>Shock index diastolique.1</th>\n",
       "      <th>GCS DCA</th>\n",
       "      <th>GCS (M) DCA</th>\n",
       "      <th>TempÃ©rature DCA</th>\n",
       "      <th>HÃ©mocue DCA</th>\n",
       "      <th>...</th>\n",
       "      <th>infratentorial_IPH</th>\n",
       "      <th>infratentorial_SAH</th>\n",
       "      <th>infratentorial_Petechiae</th>\n",
       "      <th>infratentorial_Edema</th>\n",
       "      <th>brainstem_IPH</th>\n",
       "      <th>brainstem_SAH</th>\n",
       "      <th>brainstem_Petechiae</th>\n",
       "      <th>brainstem_Edema</th>\n",
       "      <th>SDH_y</th>\n",
       "      <th>EDH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>87</td>\n",
       "      <td>132</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.52</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2476</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>35.2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>nd</td>\n",
       "      <td>38.2</td>\n",
       "      <td>12.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>11685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.33</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.83</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>796</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PAS DCA PAD DCA FC  DCA Shock index DCA Shock Index inversÃ©.1  \\\n",
       "0     120      87     132             1.1                  0.91   \n",
       "1     110      60      60            0.55                  1.83   \n",
       "2     110      50     100            0.91                   1.1   \n",
       "3     120      90     120               1                     1   \n",
       "4     120      60     110            0.92                  1.09   \n",
       "\n",
       "  Shock index diastolique.1 GCS DCA GCS (M) DCA TempÃ©rature DCA HÃ©mocue DCA  \\\n",
       "0                      1.52      15           6            37.2        13.7   \n",
       "1                         1      15           6            35.2          14   \n",
       "2                         2      13          nd            38.2        12.7   \n",
       "3                      1.33      15           6            37.8        12.8   \n",
       "4                      1.83      15           6            37.1        15.6   \n",
       "\n",
       "   ... infratentorial_IPH infratentorial_SAH infratentorial_Petechiae  \\\n",
       "0  ...                  0                  0                        0   \n",
       "1  ...                  0                 15                        0   \n",
       "2  ...                  0                  0                        0   \n",
       "3  ...                  0                  0                        0   \n",
       "4  ...                  0                  0                        0   \n",
       "\n",
       "  infratentorial_Edema  brainstem_IPH  brainstem_SAH  brainstem_Petechiae  \\\n",
       "0                    0              0              0                    0   \n",
       "1                    0              0              0                    0   \n",
       "2                    0              0              0                    0   \n",
       "3                    0              0              0                    0   \n",
       "4                    0              0              0                    0   \n",
       "\n",
       "   brainstem_Edema  SDH_y    EDH  \n",
       "0                0   2476    229  \n",
       "1                0     43      0  \n",
       "2                0    312  11685  \n",
       "3                0     11      0  \n",
       "4                0    796      0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Verify if there are NaN values\n",
    "print(f\"Number of NaN values per column:\\n{X_dca_and_segmentation.isna().sum()}\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "X_dca_and_segmentation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4ec7e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAS DCA</th>\n",
       "      <th>PAD DCA</th>\n",
       "      <th>FC  DCA</th>\n",
       "      <th>Shock index DCA</th>\n",
       "      <th>Shock Index inversÃ©.1</th>\n",
       "      <th>Shock index diastolique.1</th>\n",
       "      <th>GCS DCA</th>\n",
       "      <th>GCS (M) DCA</th>\n",
       "      <th>TempÃ©rature DCA</th>\n",
       "      <th>HÃ©mocue DCA</th>\n",
       "      <th>...</th>\n",
       "      <th>infratentorial_SAH</th>\n",
       "      <th>infratentorial_Petechiae</th>\n",
       "      <th>infratentorial_Edema</th>\n",
       "      <th>brainstem_IPH</th>\n",
       "      <th>brainstem_SAH</th>\n",
       "      <th>brainstem_Petechiae</th>\n",
       "      <th>brainstem_Edema</th>\n",
       "      <th>SDH_y</th>\n",
       "      <th>EDH</th>\n",
       "      <th>dtc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>87</td>\n",
       "      <td>132</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.52</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2476</td>\n",
       "      <td>229</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>35.2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>nd</td>\n",
       "      <td>38.2</td>\n",
       "      <td>12.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>11685</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.33</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.83</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>37.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>796</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PAS DCA PAD DCA FC  DCA Shock index DCA Shock Index inversÃ©.1  \\\n",
       "0     120      87     132             1.1                  0.91   \n",
       "1     110      60      60            0.55                  1.83   \n",
       "2     110      50     100            0.91                   1.1   \n",
       "3     120      90     120               1                     1   \n",
       "4     120      60     110            0.92                  1.09   \n",
       "\n",
       "  Shock index diastolique.1 GCS DCA GCS (M) DCA TempÃ©rature DCA HÃ©mocue DCA  \\\n",
       "0                      1.52      15           6            37.2        13.7   \n",
       "1                         1      15           6            35.2          14   \n",
       "2                         2      13          nd            38.2        12.7   \n",
       "3                      1.33      15           6            37.8        12.8   \n",
       "4                      1.83      15           6            37.1        15.6   \n",
       "\n",
       "   ... infratentorial_SAH infratentorial_Petechiae infratentorial_Edema  \\\n",
       "0  ...                  0                        0                    0   \n",
       "1  ...                 15                        0                    0   \n",
       "2  ...                  0                        0                    0   \n",
       "3  ...                  0                        0                    0   \n",
       "4  ...                  0                        0                    0   \n",
       "\n",
       "  brainstem_IPH  brainstem_SAH  brainstem_Petechiae  brainstem_Edema  SDH_y  \\\n",
       "0             0              0                    0                0   2476   \n",
       "1             0              0                    0                0     43   \n",
       "2             0              0                    0                0    312   \n",
       "3             0              0                    0                0     11   \n",
       "4             0              0                    0                0    796   \n",
       "\n",
       "     EDH  dtc  \n",
       "0    229  0.0  \n",
       "1      0  0.0  \n",
       "2  11685  1.0  \n",
       "3      0  0.0  \n",
       "4      0  0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the conditions for dtc\n",
    "def calculate_dtc(row):\n",
    "    # Condition for dtc = NA\n",
    "    if ('nr' in [row['DTC Vd'], row['DTC IP']] or\n",
    "        'non rÃ©alisÃ©' in [row['DTC Vd'], row['DTC IP']]):\n",
    "        return np.nan\n",
    "    # Condition for dtc = 1\n",
    "    if (row['DTC Vd'] == 'Pathologique' or row['DTC IP'] == 'Pathologique' or\n",
    "        (is_numeric(row['DTC Vd']) and float(row['DTC Vd']) < 30) or\n",
    "        (is_numeric(row['DTC IP']) and float(row['DTC IP']) > 1.2)):\n",
    "        return 1\n",
    "    # Default case for dtc = 0\n",
    "    return 0\n",
    "\n",
    "# Helper function to check if a value is numeric\n",
    "def is_numeric(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "# Apply the function row-wise to create the new column\n",
    "X_dca_and_segmentation['dtc'] = X_dca_and_segmentation.apply(calculate_dtc, axis=1)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "X_dca_and_segmentation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a75bab97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values per column:\n",
      "PAS DCA                       14\n",
      "PAD DCA                       14\n",
      "FC  DCA                       17\n",
      "Shock index DCA               20\n",
      "Shock Index inversÃ©.1         20\n",
      "Shock index diastolique.1     20\n",
      "GCS DCA                       30\n",
      "GCS (M) DCA                   33\n",
      "TempÃ©rature DCA              402\n",
      "HÃ©mocue DCA                   37\n",
      "Dextro DCA (mmol/l)           75\n",
      "DTC Vd                       453\n",
      "DTC IP                       452\n",
      "OsmothÃ©rapie                 395\n",
      "supratentorial_IPH             0\n",
      "supratentorial_SAH             0\n",
      "supratentorial_Petechiae       0\n",
      "supratentorial_Edema           0\n",
      "infratentorial_IPH             0\n",
      "infratentorial_SAH             0\n",
      "infratentorial_Petechiae       0\n",
      "infratentorial_Edema           0\n",
      "brainstem_IPH                  0\n",
      "brainstem_SAH                  0\n",
      "brainstem_Petechiae            0\n",
      "brainstem_Edema                0\n",
      "SDH_y                          0\n",
      "EDH                            0\n",
      "dtc                          163\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert all columns to numeric, replacing non-numeric values and empty strings with NaN\n",
    "X_dca_and_segmentation = X_dca_and_segmentation.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "X_dca_and_segmentation.head()\n",
    "\n",
    "# Verify if there are NaN values\n",
    "print(f\"Number of NaN values per column:\\n{X_dca_and_segmentation.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d4796745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name  mortality\n",
      "0  P0000          0\n",
      "1  P0001          0\n",
      "2  P0002          0\n",
      "3  P0003          0\n",
      "4  P0004          0\n",
      "Outcome events : 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bettik/PROJECTS/pr-gin5_aini/fehrdelt/FastDiag/data_preprocessing_outcomes.py:236: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col] = data[col].replace({'1': 1, '0': 0, 'nd': np.nan, '8 Upper Good Recovery (Upper GR)': np.nan}).astype(float)\n",
      "/bettik/PROJECTS/pr-gin5_aini/fehrdelt/FastDiag/data_preprocessing_outcomes.py:236: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col] = data[col].replace({'1': 1, '0': 0, 'nd': np.nan, '8 Upper Good Recovery (Upper GR)': np.nan}).astype(float)\n",
      "/bettik/PROJECTS/pr-gin5_aini/fehrdelt/FastDiag/data_preprocessing_outcomes.py:236: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col] = data[col].replace({'1': 1, '0': 0, 'nd': np.nan, '8 Upper Good Recovery (Upper GR)': np.nan}).astype(float)\n"
     ]
    }
   ],
   "source": [
    "# Include column name\n",
    "X_dca_and_segmentation = pd.concat([X_dca_and_segmentation, data_full.iloc[:, 2]], axis=1)\n",
    "X = X_dca_and_segmentation.copy()\n",
    "y = get_mortality_6m()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9a3038b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes with NaN in 'mortality': Index([], dtype='int64')\n",
      "Shape of y after cleaning: (566, 2)\n",
      "Shape of X after cleaning: (534, 30)\n",
      "Unique 'name' values in y: 562\n",
      "Unique 'name' values in X: 534\n",
      "Names in y but not in X: {'P0349', 'P0518', 'P0155', 'P0150', 'P0383', 'P0224', 'P0217', 'P0564', 'P0532', 'P0327', 'P0147', 'P0207', 'P0018', 'P0195', 'P0430', 'P0239', 'P0149', 'P0480', 'P0083', 'P0369', 'P0422', 'P0412', 'P0111', 'P0356', 'P0010', 'P0527', 'P0343', 'P0491'}\n",
      "Names in X but not in y: set()\n",
      "Shape of y after alignment: (536, 2)\n",
      "Shape of X after alignment: (534, 30)\n",
      "Number of duplicate names in y: 2\n",
      "Number of duplicate names in X: 0\n",
      "Shape of y after removing duplicates: (534, 2)\n",
      "Shape of X after removing duplicates: (534, 30)\n",
      "Are 'name' values aligned between y and X True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Identify indexes where 'mortality' is NaN in y\n",
    "nan_indexes = y.loc[pd.isna(y[\"mortality\"]), :].index\n",
    "print(f\"Indexes with NaN in 'mortality': {nan_indexes}\")\n",
    "\n",
    "# Drop rows with NaN in y\n",
    "y = y.drop(index=nan_indexes)\n",
    "\n",
    "# Ensure 'name' is the index or used for alignment\n",
    "X = X[~X['name'].isin(y.loc[nan_indexes, 'name'])]\n",
    "\n",
    "# Verify the shapes after cleaning\n",
    "print(f\"Shape of y after cleaning: {y.shape}\")\n",
    "print(f\"Shape of X after cleaning: {X.shape}\")\n",
    "\n",
    "# VÃ©rifiez les valeurs uniques de 'name' dans y\n",
    "print(\"Unique 'name' values in y:\", y['name'].nunique())\n",
    "\n",
    "# VÃ©rifiez les valeurs uniques de 'name' dans X\n",
    "print(\"Unique 'name' values in X:\", X['name'].nunique())\n",
    "\n",
    "# Noms dans y mais pas dans X\n",
    "missing_in_X = set(y['name']) - set(X['name'])\n",
    "print(f\"Names in y but not in X: {missing_in_X}\")\n",
    "\n",
    "# Noms dans X mais pas dans y\n",
    "missing_in_y = set(X['name']) - set(y['name'])\n",
    "print(f\"Names in X but not in y: {missing_in_y}\")\n",
    "\n",
    "# Trouver les noms communs\n",
    "common_names = set(y['name']).intersection(set(X['name']))\n",
    "\n",
    "# Filtrer y et X pour ne garder que les noms communs\n",
    "y = y[y['name'].isin(common_names)]\n",
    "X = X[X['name'].isin(common_names)]\n",
    "\n",
    "# VÃ©rifiez les dimensions aprÃ¨s nettoyage\n",
    "print(f\"Shape of y after alignment: {y.shape}\")\n",
    "print(f\"Shape of X after alignment: {X.shape}\")\n",
    "\n",
    "# Check for duplicates in 'name' in y\n",
    "print(\"Number of duplicate names in y:\", y['name'].duplicated().sum())\n",
    "\n",
    "# Check for duplicates in 'name' in X\n",
    "print(\"Number of duplicate names in X:\", X['name'].duplicated().sum())\n",
    "\n",
    "# Remove duplicates from both DataFrames\n",
    "y = y.drop_duplicates(subset=['name'])\n",
    "X = X.drop_duplicates(subset=['name'])\n",
    "\n",
    "# Verify the shapes again\n",
    "print(f\"Shape of y after removing duplicates: {y.shape}\")\n",
    "print(f\"Shape of X after removing duplicates: {X.shape}\")\n",
    "\n",
    "# Check if 'name' values are the same in both DataFrames\n",
    "common_names = set(y['name']) == set(X['name'])\n",
    "print(f\"Are 'name' values aligned between y and X {common_names}\")\n",
    "\n",
    "# Trier y et X par la colonne 'name'\n",
    "y = y.sort_values(by='name').reset_index(drop=True)\n",
    "X = X.sort_values(by='name').reset_index(drop=True)\n",
    "\n",
    "# VÃ©rifier si les noms sont alignÃ©s\n",
    "print((y['name'].values == X['name'].values).all())\n",
    "\n",
    "# Drop the 'name' column from X\n",
    "#X_features = X.drop(columns=['name', 'mortalitÃ© J7'])\n",
    "X_features = X.drop(columns=['name'])\n",
    "\n",
    "# imputation with median \n",
    "X_features_imputed = X_features.fillna(X_features.median())\n",
    "\n",
    "# Drop the 'name' column from y (if it exists)\n",
    "y_outcome = y.drop(columns=['name'])\n",
    "\n",
    "# Ensure y_outcome is a 1D array\n",
    "y_outcome = y_outcome.values.ravel()  # Convert to 1D if using pandas DataFrame\n",
    "\n",
    "FOLDS = 5\n",
    "N_REPEATS = 3\n",
    "nb_total_samples = len(y_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7dca7dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAS DCA</th>\n",
       "      <th>PAD DCA</th>\n",
       "      <th>FC  DCA</th>\n",
       "      <th>Shock index DCA</th>\n",
       "      <th>Shock Index inversÃ©.1</th>\n",
       "      <th>Shock index diastolique.1</th>\n",
       "      <th>GCS DCA</th>\n",
       "      <th>GCS (M) DCA</th>\n",
       "      <th>TempÃ©rature DCA</th>\n",
       "      <th>HÃ©mocue DCA</th>\n",
       "      <th>...</th>\n",
       "      <th>infratentorial_SAH</th>\n",
       "      <th>infratentorial_Petechiae</th>\n",
       "      <th>infratentorial_Edema</th>\n",
       "      <th>brainstem_IPH</th>\n",
       "      <th>brainstem_SAH</th>\n",
       "      <th>brainstem_Petechiae</th>\n",
       "      <th>brainstem_Edema</th>\n",
       "      <th>SDH_y</th>\n",
       "      <th>EDH</th>\n",
       "      <th>dtc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.52</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2476</td>\n",
       "      <td>229</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>35.2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.2</td>\n",
       "      <td>12.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>11685</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.33</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>37.8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.83</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>37.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>796</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PAS DCA  PAD DCA  FC  DCA  Shock index DCA  Shock Index inversÃ©.1  \\\n",
       "0    120.0     87.0    132.0             1.10                   0.91   \n",
       "1    110.0     60.0     60.0             0.55                   1.83   \n",
       "2    110.0     50.0    100.0             0.91                   1.10   \n",
       "3    120.0     90.0    120.0             1.00                   1.00   \n",
       "4    120.0     60.0    110.0             0.92                   1.09   \n",
       "\n",
       "   Shock index diastolique.1  GCS DCA  GCS (M) DCA  TempÃ©rature DCA  \\\n",
       "0                       1.52     15.0          6.0             37.2   \n",
       "1                       1.00     15.0          6.0             35.2   \n",
       "2                       2.00     13.0          NaN             38.2   \n",
       "3                       1.33     15.0          6.0             37.8   \n",
       "4                       1.83     15.0          6.0             37.1   \n",
       "\n",
       "   HÃ©mocue DCA  ...  infratentorial_SAH  infratentorial_Petechiae  \\\n",
       "0         13.7  ...                   0                         0   \n",
       "1         14.0  ...                  15                         0   \n",
       "2         12.7  ...                   0                         0   \n",
       "3         12.8  ...                   0                         0   \n",
       "4         15.6  ...                   0                         0   \n",
       "\n",
       "   infratentorial_Edema  brainstem_IPH  brainstem_SAH  brainstem_Petechiae  \\\n",
       "0                     0              0              0                    0   \n",
       "1                     0              0              0                    0   \n",
       "2                     0              0              0                    0   \n",
       "3                     0              0              0                    0   \n",
       "4                     0              0              0                    0   \n",
       "\n",
       "   brainstem_Edema  SDH_y    EDH  dtc  \n",
       "0                0   2476    229  0.0  \n",
       "1                0     43      0  0.0  \n",
       "2                0    312  11685  1.0  \n",
       "3                0     11      0  0.0  \n",
       "4                0    796      0  0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dd31f236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoostingClassifier with hyperparameter gridsearch\n",
      "AUC (max): 0.89 +- 0.06\n",
      "F1 Score (max): 0.46 +- 0.11\n",
      "F2 Score (max): 0.53 +- 0.14\n",
      "Brier Score (min): 0.07 +- -0.02\n",
      "False negative: 2% +- 1\n",
      "False positive: 7% +- 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline_smote_under = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf = GridSearchCV(estimator=pipeline_smote_under, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv)\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling = cross_validate(clf, X_features_imputed, y_outcome, \n",
    "                                                scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, \n",
    "                                                cv=outer_cv, n_jobs=-1, return_estimator=True, return_indices=True)\n",
    "\n",
    "print(\"HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf37fcb",
   "metadata": {},
   "source": [
    "#### Confidence intervals with test set bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5db9fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_list = []\n",
    "\n",
    "for estimator in nested_scores_smote_undersampling[\"estimator\"]:\n",
    "    best_params_list.append(frozenset(estimator.best_params_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "90e4a06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final chosen hyperparameters: {'over__sampling_strategy': 0.2, 'under__sampling_strategy': 0.7, 'model__learning_rate': 1, 'over__k_neighbors': 5}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Convert dict to a hashable tuple and count occurrences\n",
    "most_common_params = Counter(best_params_list).most_common(1)[0][0]\n",
    "final_params = dict(most_common_params)  # Convert back to dict\n",
    "\n",
    "\n",
    "print(\"Final chosen hyperparameters:\", final_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "05b8b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('over', SMOTE(k_neighbors=5, sampling_strategy=0.2)), ('under', RandomUnderSampler(sampling_strategy=0.7)), ('model', HistGradientBoostingClassifier(learning_rate=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2a1bf9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(pipeline, X_features_imputed, y_outcome, cv=20, method='predict_proba')[:,1]\n",
    "\n",
    "y_pred_binary = [1 if i>=0.5 else 0 for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62f8a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, confusion_matrix, roc_auc_score, f1_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2961b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=12345)\n",
    "idx = np.arange(len(y_outcome))\n",
    "\n",
    "y_pred = np.asarray(y_pred)\n",
    "y_pred_binary = np.asarray(y_pred_binary)\n",
    "y = np.asarray(y_outcome)\n",
    "X_dropped = np.asarray(X_features_imputed)\n",
    "\n",
    "test_roc_auc = []\n",
    "test_f1 = []\n",
    "test_ftwos = []\n",
    "test_brier = []\n",
    "test_false_neg = []\n",
    "test_false_pos = []\n",
    "\n",
    "for i in range(200): # bootstrap with 200 rounds: random sampling with replacement of the predictions\n",
    "\n",
    "    pred_idx = rng.choice(idx, size=len(idx), replace=True)\n",
    "    \n",
    "    roc_auc_test_boot = roc_auc_score(y_score=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    f1_test_boot = f1_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx])\n",
    "    f2_test_boot = fbeta_score(y_pred=y_pred_binary[pred_idx], y_true=y[pred_idx], beta=2)\n",
    "    brier_test_boot = brier_score_loss(y_proba=y_pred[pred_idx], y_true=y[pred_idx])\n",
    "    false_neg_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[1,0]\n",
    "    false_pos_test_boot = confusion_matrix(y[pred_idx], y_pred_binary[pred_idx])[0,1]\n",
    "    \n",
    "    test_roc_auc.append(roc_auc_test_boot)\n",
    "    test_f1.append(f1_test_boot)\n",
    "    test_ftwos.append(f2_test_boot)\n",
    "    test_brier.append(brier_test_boot)\n",
    "    test_false_neg.append(false_neg_test_boot/len(idx)*100)\n",
    "    test_false_pos.append(false_pos_test_boot/len(idx)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fe02f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification performance\n",
      "\n",
      "ROC AUC:         0.88;   95% CI 0.81-0.94\n",
      "F1:              0.55;   95% CI 0.42-0.67\n",
      "F2:              0.61;   95% CI 0.47-0.74\n",
      "Brier loss:      0.06;   95% CI 0.04-0.08\n",
      "False negatives: 2.06%;  95% CI 1.12-3.19\n",
      "False negatives: 4.36%; 95% CI 2.43-6.37\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification performance\\n\")\n",
    "\n",
    "bootstrap_roc_auc_test_mean = np.mean(test_roc_auc)\n",
    "ci_lower = np.percentile(test_roc_auc, 2.5)     # 2.5 percentile (alpha=0.025)\n",
    "ci_upper = np.percentile(test_roc_auc, 97.5)\n",
    "print(f\"ROC AUC:         {bootstrap_roc_auc_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f1_test_mean = np.mean(test_f1)\n",
    "ci_lower = np.percentile(test_f1, 2.5)\n",
    "ci_upper = np.percentile(test_f1, 97.5)\n",
    "print(f\"F1:              {bootstrap_f1_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_f2_test_mean = np.mean(test_ftwos)\n",
    "ci_lower = np.percentile(test_ftwos, 2.5)\n",
    "ci_upper = np.percentile(test_ftwos, 97.5)\n",
    "print(f\"F2:              {bootstrap_f2_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_brier_test_mean = np.mean(test_brier)\n",
    "ci_lower = np.percentile(test_brier, 2.5)\n",
    "ci_upper = np.percentile(test_brier, 97.5)\n",
    "print(f\"Brier loss:      {bootstrap_brier_test_mean:.2f};   95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_neg_test_mean = np.mean(test_false_neg)\n",
    "ci_lower = np.percentile(test_false_neg, 2.5)\n",
    "ci_upper = np.percentile(test_false_neg, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_neg_test_mean:.2f}%;  95% CI {ci_lower:.2f}-{ci_upper:.2f}\")\n",
    "\n",
    "bootstrap_false_pos_test_mean = np.mean(test_false_pos)\n",
    "ci_lower = np.percentile(test_false_pos, 2.5)\n",
    "ci_upper = np.percentile(test_false_pos, 97.5)\n",
    "print(f\"False negatives: {bootstrap_false_pos_test_mean:.2f}%; 95% CI {ci_lower:.2f}-{ci_upper:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16515c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
