{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "def confusion_matrix_scorer(clf, X, y):\n",
    "\n",
    "     y_pred = clf.predict(X)\n",
    "     cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "     return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
    "             'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "\n",
    "def false_neg_scorer(clf, X, y):\n",
    "\n",
    "     y_pred = clf.predict(X)\n",
    "     cm = confusion_matrix(y, y_pred)\n",
    "     \n",
    "     return cm[1, 0]\n",
    "\n",
    "def false_pos_scorer(clf, X, y):\n",
    "\n",
    "     y_pred = clf.predict(X)\n",
    "     cm = confusion_matrix(y, y_pred)\n",
    "     \n",
    "     return cm[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "N_REPEATS = 3\n",
    "nb_total_samples = len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_full = pd.read_csv(DATA_DIRECTORY+\"cleaned_data_full_bis.csv\", skiprows=1)\n",
    "cleaned_data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_volumes = pd.read_csv(DATA_DIRECTORY+\"cleaned_data_full_bis.csv\", usecols=range(71,79), skiprows=1)\n",
    "X_volumes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marshal preprocessing\n",
    "X_volumes['Marshal'] = X_volumes['Marshal'].replace({\n",
    "    \"I diffuse injury (no visible pathology)\": 1,\n",
    "    \"II diffuse injury (midline shift <5mm, basal cisterns visible,no high or mixed density lesion > 25 cm3)\": 2,\n",
    "    \"III diffuse injury (swelling, midline shift of 0 to 5 mm, basal cisterns compressed or completely effaced, no high or mixed density lesions >25 cm3)\": 3,\n",
    "    \"IV diffuse injury (midline shift >5 mm, no high or mixed density lesions >25 cm3)\": 4,\n",
    "    \"IVdiffuse injury (midline shift >5 mm, no high or mixed density lesions >25 cm3)\": 4,\n",
    "    \"V evacuated mass lesion (any lesion evacuated surgically)\": 5,\n",
    "    \"VI non-evacuated mass lesion (high or mixed density lesions >25 cm3, not surgically evacuated)\": 6, \n",
    "    \"0\": 0,  # if\"0\" is an object\n",
    "    \"1\": 1,  \n",
    "    \"2\": 2,\n",
    "    \"3\": 3,\n",
    "    \"4\": 4,\n",
    "    \"5\": 5,\n",
    "    \"6\": 6\n",
    "})\n",
    "\n",
    "# Replace NA or 0 by 1\n",
    "X_volumes['Marshal'] = X_volumes['Marshal'].apply(lambda x: 1 if pd.isna(x) or x == 0 else x)\n",
    "\n",
    "print(X_volumes['Marshal'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(DATA_DIRECTORY+\"cleaned_data_full_bis.csv\", skiprows=1, usecols=[90])\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_volumes_imputed = X_volumes.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mortalité J7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_and_nd_indexes = y.loc[(pd.isna(y[\"mortalité J7\"])) | (y[\"mortalité J7\"] == \"nd\"), :].index  # indexes where there is a nan value.\n",
    "print(nan_and_nd_indexes)\n",
    "\n",
    "y = y.drop(nan_and_nd_indexes)\n",
    "X_volumes = X_volumes.drop(nan_and_nd_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert in number\n",
    "y[\"mortalité J7\"] = pd.to_numeric(y[\"mortalité J7\"], errors=\"coerce\") \n",
    "\n",
    "# Outcome event\n",
    "event_count = (y == 1.00).sum()\n",
    "print(f\"outcome events : {event_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y['mortalité J7'].to_numpy()\n",
    "y = [int(i) for i in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB with hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_smote_under = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf = GridSearchCV(estimator=pipeline_smote_under, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv)\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling = cross_validate(clf, X_volumes_imputed, y, scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, cv=outer_cv, n_jobs=-1)\n",
    "\n",
    "print(\"Prehospital data & segmentation volumes: HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mortalité J30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load columns mortality\n",
    "mortality_column = pd.read_csv(DATA_DIRECTORY + \"cleaned_data_full.csv\", skiprows=1, usecols=[95, 96, 97])\n",
    "\n",
    "# If column mortalité J7 has a 1, set column mortalité J30 to 1; otherwise, leave column mortalité J30 as is\n",
    "mortality_column.iloc[:, 1] = mortality_column.apply(\n",
    "    lambda row: 1 if str(row.iloc[0]).strip() == \"1\" else row.iloc[1], axis=1\n",
    ")\n",
    "\n",
    "# Drop column 95, keeping only column 96\n",
    "y = mortality_column.iloc[:, [1]]\n",
    "\n",
    "# Convert in number\n",
    "y[\"mortalité J30\"] = pd.to_numeric(y[\"mortalité J30\"], errors=\"coerce\") \n",
    "\n",
    "# Outcome event\n",
    "event_count = (y == 1).sum()\n",
    "print(f\"outcome events : {event_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_and_nd_indexes = y.loc[(pd.isna(y[\"mortalité J30\"])) | (y[\"mortalité J30\"] == \"nd\"), :].index  # indexes where there is a nan value.\n",
    "print(nan_indexes)\n",
    "\n",
    "y = y.drop(nan_and_nd_indexes)\n",
    "X_volumes = X_volumes.drop(nan_and_nd_indexes)\n",
    "\n",
    "y = y['mortalité J30'].to_numpy()\n",
    "y = [int(i) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XGB ###\n",
    "\n",
    "pipeline_smote_under = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf = GridSearchCV(estimator=pipeline_smote_under, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv)\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling = cross_validate(clf, X_volumes_imputed, y, scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, cv=outer_cv, n_jobs=-1)\n",
    "\n",
    "print(\"Prehospital data & segmentation volumes: HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mortalité 6 mois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(DATA_DIRECTORY+\"cleaned_data_full_bis.csv\", skiprows=1, usecols=[92])\n",
    "y.head()\n",
    "\n",
    "X_volumes = pd.read_csv(DATA_DIRECTORY+\"cleaned_data_full_bis.csv\", usecols=range(71,79), skiprows=1)\n",
    "X_volumes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marshal preprocessing\n",
    "X_volumes['Marshal'] = X_volumes['Marshal'].replace({\n",
    "    \"I diffuse injury (no visible pathology)\": 1,\n",
    "    \"II diffuse injury (midline shift <5mm, basal cisterns visible,no high or mixed density lesion > 25 cm3)\": 2,\n",
    "    \"III diffuse injury (swelling, midline shift of 0 to 5 mm, basal cisterns compressed or completely effaced, no high or mixed density lesions >25 cm3)\": 3,\n",
    "    \"IV diffuse injury (midline shift >5 mm, no high or mixed density lesions >25 cm3)\": 4,\n",
    "    \"IVdiffuse injury (midline shift >5 mm, no high or mixed density lesions >25 cm3)\": 4,\n",
    "    \"V evacuated mass lesion (any lesion evacuated surgically)\": 5,\n",
    "    \"VI non-evacuated mass lesion (high or mixed density lesions >25 cm3, not surgically evacuated)\": 6, \n",
    "    \"0\": 0,  # if\"0\" is an object\n",
    "    \"1\": 1,  \n",
    "    \"2\": 2,\n",
    "    \"3\": 3,\n",
    "    \"4\": 4,\n",
    "    \"5\": 5,\n",
    "    \"6\": 6\n",
    "})\n",
    "\n",
    "# Replace NA or 0 by 1\n",
    "X_volumes['Marshal'] = X_volumes['Marshal'].apply(lambda x: 1 if pd.isna(x) or x == 0 else x)\n",
    "\n",
    "print(X_volumes['Marshal'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_and_nd_indexes = y.loc[(pd.isna(y[\"Mortalité 6 mois\"])) | (y[\"Mortalité 6 mois\"] == \"nd\"), :].index  # indexes where there is a nan value.\n",
    "print(nan_and_nd_indexes)\n",
    "\n",
    "y = y.drop(nan_and_nd_indexes)\n",
    "X_volumes = X_volumes.drop(nan_and_nd_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert in number\n",
    "y[\"Mortalité 6 mois\"] = pd.to_numeric(y[\"Mortalité 6 mois\"], errors=\"coerce\") \n",
    "\n",
    "# Outcome event\n",
    "event_count = (y == 1.00).sum()\n",
    "print(f\"outcome events : {event_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y['Mortalité 6 mois'].to_numpy()\n",
    "y = [int(i) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_volumes_imputed = X_volumes.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[\"Mortalité 6 mois\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_smote_under = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf = GridSearchCV(estimator=pipeline_smote_under, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv)\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling = cross_validate(clf, X_volumes_imputed, y, scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, cv=outer_cv, n_jobs=-1)\n",
    "\n",
    "print(\"segmentation volumes: HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TILSUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_volumes = pd.read_csv(DATA_DIRECTORY+\"cleaned_data_full_bis.csv\", usecols=range(71,79), skiprows=1)\n",
    "X_volumes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marshal preprocessing\n",
    "X_volumes['Marshal'] = X_volumes['Marshal'].replace({\n",
    "    \"I diffuse injury (no visible pathology)\": 1,\n",
    "    \"II diffuse injury (midline shift <5mm, basal cisterns visible,no high or mixed density lesion > 25 cm3)\": 2,\n",
    "    \"III diffuse injury (swelling, midline shift of 0 to 5 mm, basal cisterns compressed or completely effaced, no high or mixed density lesions >25 cm3)\": 3,\n",
    "    \"IV diffuse injury (midline shift >5 mm, no high or mixed density lesions >25 cm3)\": 4,\n",
    "    \"IVdiffuse injury (midline shift >5 mm, no high or mixed density lesions >25 cm3)\": 4,\n",
    "    \"V evacuated mass lesion (any lesion evacuated surgically)\": 5,\n",
    "    \"VI non-evacuated mass lesion (high or mixed density lesions >25 cm3, not surgically evacuated)\": 6, \n",
    "    \"0\": 0,  # if\"0\" is an object\n",
    "    \"1\": 1,  \n",
    "    \"2\": 2,\n",
    "    \"3\": 3,\n",
    "    \"4\": 4,\n",
    "    \"5\": 5,\n",
    "    \"6\": 6\n",
    "})\n",
    "\n",
    "# Replace NA or 0 by 1\n",
    "X_volumes['Marshal'] = X_volumes['Marshal'].apply(lambda x: 1 if pd.isna(x) or x == 0 else x)\n",
    "\n",
    "print(X_volumes['Marshal'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIL = pd.read_csv(DATA_DIRECTORY+\"cleaned_data_full_bis.csv\", usecols=range(66,71), skiprows=1)\n",
    "TIL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create y based on the conditions: TIL 2 = 1 or TIL 3 = 1 or TIL 4 = 1\n",
    "y = pd.DataFrame()\n",
    "y[\"y\"] = ((TIL.iloc[:, 2] == 1) | (TIL.iloc[:, 3] == 1) | (TIL.iloc[:, 4] == 1)).astype(int)\n",
    "\n",
    "# Verify the first few rows of y\n",
    "print(y.head())\n",
    "\n",
    "# Outcome event\n",
    "event_count = (y == 1.00).sum()\n",
    "print(f\"outcome events : {event_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create y based on the conditions and propagate NA values\n",
    "y = pd.DataFrame(index=TIL.index)  # Keep the same indexing as TIL\n",
    "\n",
    "# Apply the conditions, setting NA in y if there are any NA values in the relevant TIL columns\n",
    "y[\"y\"] = TIL.iloc[:, [0, 1, 2, 3, 4]].apply(\n",
    "    lambda row: 1 if (row.iloc[2] == 1 or row.iloc[3] == 1 or row.iloc[4] == 1) else 0, axis=1\n",
    ")\n",
    "\n",
    "# Set y to NaN if any NA exists in the relevant columns\n",
    "y.loc[TIL.iloc[:, [0, 1, 2, 3, 4]].isnull().any(axis=1), \"y\"] = pd.NA\n",
    "\n",
    "# Verify the first few rows of y\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_and_nd_indexes = y.loc[y.isna().any(axis=1)].index  # Get indexes where any NaN exists\n",
    "print(nan_and_nd_indexes)\n",
    "\n",
    "y = y.drop(nan_and_nd_indexes)\n",
    "X_volumes = X_volumes.drop(nan_and_nd_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_volumes_imputed = X_volumes.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.to_numpy().ravel()  # Convert y to a 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y), y.shape)  # Type should be numpy.ndarray and shape should be (n_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_smote_under = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf = GridSearchCV(estimator=pipeline_smote_under, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv)\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling = cross_validate(clf, X_volumes_imputed, y, scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, cv=outer_cv, n_jobs=-1)\n",
    "\n",
    "print(\"segmentation volumes: HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_volumes = pd.read_csv(DATA_DIRECTORY+\"cleaned_data_full_bis.csv\", usecols=range(71,79), skiprows=1)\n",
    "X_volumes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marshal preprocessing\n",
    "X_volumes['Marshal'] = X_volumes['Marshal'].replace({\n",
    "    \"I diffuse injury (no visible pathology)\": 1,\n",
    "    \"II diffuse injury (midline shift <5mm, basal cisterns visible,no high or mixed density lesion > 25 cm3)\": 2,\n",
    "    \"III diffuse injury (swelling, midline shift of 0 to 5 mm, basal cisterns compressed or completely effaced, no high or mixed density lesions >25 cm3)\": 3,\n",
    "    \"IV diffuse injury (midline shift >5 mm, no high or mixed density lesions >25 cm3)\": 4,\n",
    "    \"IVdiffuse injury (midline shift >5 mm, no high or mixed density lesions >25 cm3)\": 4,\n",
    "    \"V evacuated mass lesion (any lesion evacuated surgically)\": 5,\n",
    "    \"VI non-evacuated mass lesion (high or mixed density lesions >25 cm3, not surgically evacuated)\": 6, \n",
    "    \"0\": 0,  # if\"0\" is an object\n",
    "    \"1\": 1,  \n",
    "    \"2\": 2,\n",
    "    \"3\": 3,\n",
    "    \"4\": 4,\n",
    "    \"5\": 5,\n",
    "    \"6\": 6\n",
    "})\n",
    "\n",
    "# Replace NA or 0 by 1\n",
    "X_volumes['Marshal'] = X_volumes['Marshal'].apply(lambda x: 1 if pd.isna(x) or x == 0 else x)\n",
    "\n",
    "print(X_volumes['Marshal'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIER = pd.read_csv(DATA_DIRECTORY+\"cleaned_data_full_bis.csv\", usecols=range(55,65), skiprows=1)\n",
    "TIER.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create y based on the conditions and propagate NA values\n",
    "y = pd.DataFrame(index=TIER.index)  # Keep the same indexing as TIL\n",
    "\n",
    "# Apply the conditions, setting NA in y if there are any NA values in the relevant TIL columns\n",
    "y[\"y\"] = TIER.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]].apply(\n",
    "    lambda row: 1 if (row.iloc[6] == 1 or row.iloc[7] == 1 or row.iloc[8] == 1 or row.iloc[9] == 1) else 0, axis=1\n",
    ")\n",
    "\n",
    "# Set y to NaN if any NA exists in the relevant columns\n",
    "y.loc[TIER.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]].isnull().any(axis=1), \"y\"] = pd.NA\n",
    "\n",
    "# Verify the first few rows of y\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_and_nd_indexes = y.loc[y.isna().any(axis=1)].index  # Get indexes where any NaN exists\n",
    "print(nan_and_nd_indexes)\n",
    "\n",
    "y = y.drop(nan_and_nd_indexes)\n",
    "X_volumes = X_volumes.drop(nan_and_nd_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome event\n",
    "event_count = (y == 1.00).sum()\n",
    "print(f\"outcome events : {event_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_volumes_imputed = X_volumes.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.to_numpy().ravel()  # Convert y to a 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y), y.shape)  # Type should be numpy.ndarray and shape should be (n_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_smote_under = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf = GridSearchCV(estimator=pipeline_smote_under, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv)\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling = cross_validate(clf, X_volumes_imputed, y, scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, cv=outer_cv, n_jobs=-1)\n",
    "\n",
    "print(\"segmentation volumes: HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
