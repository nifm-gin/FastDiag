{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "def confusion_matrix_scorer(clf, X, y):\n",
    "\n",
    "     y_pred = clf.predict(X)\n",
    "     cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "     return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
    "             'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "\n",
    "def false_neg_scorer(clf, X, y):\n",
    "\n",
    "     y_pred = clf.predict(X)\n",
    "     cm = confusion_matrix(y, y_pred)\n",
    "     \n",
    "     return cm[1, 0]\n",
    "\n",
    "def false_pos_scorer(clf, X, y):\n",
    "\n",
    "     y_pred = clf.predict(X)\n",
    "     cm = confusion_matrix(y, y_pred)\n",
    "     \n",
    "     return cm[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_full = pd.read_csv(DATA_DIRECTORY+\"cleaned_data_full.csv\", skiprows=1)\n",
    "#cleaned_data_full = pd.read_csv(DATA_DIRECTORY+\"clinical_data_anonymized.csv\", skiprows=1)\n",
    "cleaned_data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude rows where the 'exclusion' column is not null\n",
    "cleaned_data_full = cleaned_data_full[cleaned_data_full['Exclusion'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only columns 16 to 36\n",
    "X_prehosp = cleaned_data_full.iloc[:, 16:36]\n",
    "X_prehosp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all variables in X_prehosp to numeric, coercing invalid entries to NaN\n",
    "X_prehosp_numeric = X_prehosp.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Count missing values (NA) for each variable in X_prehosp\n",
    "na_counts = X_prehosp_numeric.isna().sum()\n",
    "\n",
    "# Get unique values for each variable in X_prehosp to check for potential outliers\n",
    "unique_values = {col: X_prehosp_numeric[col].unique() for col in X_prehosp_numeric.columns}\n",
    "\n",
    "# Calculate min and max for each variable in X_prehosp_numeric\n",
    "min_values = X_prehosp_numeric.min()\n",
    "max_values = X_prehosp_numeric.max()\n",
    "\n",
    "# Create the summary DataFrame with min, max, missing values, and unique values\n",
    "summary = pd.DataFrame({\n",
    "    \"Variable\": X_prehosp_numeric.columns,\n",
    "    \"Missing Values\": na_counts,\n",
    "    \"Unique Values\": [list(unique_values[col]) for col in X_prehosp_numeric.columns],\n",
    "    \"Min Value\": min_values,\n",
    "    \"Max Value\": max_values\n",
    "})\n",
    "\n",
    "# Display the summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_prehosp_numeric.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les limites maximales pour les colonnes\n",
    "capping_limits = {\n",
    "    \"Shock Index SMUR\": 3,\n",
    "    \"GCS SMUR \": 15,\n",
    "    \"GCS (M) SMUR \": 6,\n",
    "    \"Shock Index inversé\": 3,\n",
    "    \"Shock index diastolique\": 3,\n",
    "    \"Amputation\": 1,\n",
    "    \"ACR SMUR\": 1,\n",
    "    \"Hémorragie ext SMUR\": 1,\n",
    "    \"Ischémie\": 1,\n",
    "    \"Intubation prehosp\": 1,\n",
    "    \"OsmoTH prehosp\": 1,\n",
    "    \"Vasopresseur prehosp\": 1\n",
    "}\n",
    "\n",
    "# Appliquer le capping\n",
    "for column, max_value in capping_limits.items():\n",
    "    if column in X_prehosp_numeric.columns:\n",
    "        X_prehosp_numeric[column] = X_prehosp_numeric[column].clip(upper=max_value)\n",
    "    else:\n",
    "        print(f\"Warning: Column '{column}' not found in DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les stratégies d’imputation\n",
    "imputation_strategies = {\n",
    "    \"PAS  SMUR \": \"median\",\n",
    "    \"PAD  SMUR \": \"median\",\n",
    "    \"FC SMUR \": \"median\",\n",
    "    \"Shock Index SMUR\": \"median\",\n",
    "    \"GCS SMUR \": \"median\",\n",
    "    \"GCS (M) SMUR \": \"median\",\n",
    "    \"Shock Index inversé\": \"median\",\n",
    "    \"Shock index diastolique\": \"median\",\n",
    "    \"Anomalie pupille SMUR\": 0,\n",
    "    \"Fracas bassin\": 0,\n",
    "    \"Amputation\": 0,\n",
    "    \"ACR SMUR\": 0,\n",
    "    \"Hémorragie ext SMUR\": 0,\n",
    "    \"Ischémie\": 0,\n",
    "    \"Intubation prehosp\": 0,\n",
    "    \"Expansion volémique\": \"median\",\n",
    "    \"OsmoTH prehosp\": 0,\n",
    "    \"Vasopresseur prehosp\": 0\n",
    "}\n",
    "\n",
    "# Appliquer l’imputation\n",
    "for column, strategy in imputation_strategies.items():\n",
    "    if column in X_prehosp_numeric.columns:\n",
    "        if strategy == \"median\":\n",
    "            X_prehosp_numeric[column] = X_prehosp_numeric[column].fillna(X_prehosp_numeric[column].median())\n",
    "        else:\n",
    "            X_prehosp_numeric[column] = X_prehosp_numeric[column].fillna(strategy)\n",
    "    else:\n",
    "        print(f\"Warning: Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "# Vérifiez les résultats de l’imputation\n",
    "print(\"Imputation terminée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts = X_prehosp_numeric.isna().sum()\n",
    "print(na_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes à supprimer\n",
    "columns_to_drop = [\"FR SMUR\", \"Hémocue SMUR \"]\n",
    "\n",
    "# Suppression si elles existent dans le DataFrame\n",
    "columns_existing = [col for col in columns_to_drop if col in X_prehosp_numeric.columns]\n",
    "X_prehosp_numeric = X_prehosp_numeric.drop(columns=columns_existing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TILSUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIL = pd.read_csv(DATA_DIRECTORY+\"cleaned_data_full.csv\", usecols=range(71,76))\n",
    "TIL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create y based on the conditions: TIL 2 = 1 or TIL 3 = 1 or TIL 4 = 1\n",
    "y = pd.DataFrame()\n",
    "y[\"y\"] = ((TIL.iloc[:, 2] == 1) | (TIL.iloc[:, 3] == 1) | (TIL.iloc[:, 4] == 1)).astype(int)\n",
    "\n",
    "# Verify the first few rows of y\n",
    "print(y.head())\n",
    "\n",
    "# Outcome event\n",
    "event_count = (y == 1.00).sum()\n",
    "print(f\"outcome events : {event_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create y based on the conditions and propagate NA values\n",
    "y = pd.DataFrame(index=TIL.index)  # Keep the same indexing as TIL\n",
    "\n",
    "# Apply the conditions, setting NA in y if there are any NA values in the relevant TIL columns\n",
    "y[\"y\"] = TIL.iloc[:, [0, 1, 2, 3, 4]].apply(\n",
    "    lambda row: 1 if (row.iloc[2] == 1 or row.iloc[3] == 1 or row.iloc[4] == 1) else 0, axis=1\n",
    ")\n",
    "\n",
    "# Set y to NaN if any NA exists in the relevant columns\n",
    "y.loc[TIL.iloc[:, [0, 1, 2, 3, 4]].isnull().any(axis=1), \"y\"] = pd.NA\n",
    "\n",
    "# Verify the first few rows of y\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align indexes between y and X_prehosp_numeric\n",
    "X_prehosp_numeric, y = X_prehosp_numeric.align(y, join=\"inner\", axis=0)\n",
    "\n",
    "# Identify rows where any NaN exists in y\n",
    "nan_and_nd_indexes = y.loc[y.isna().any(axis=1)].index\n",
    "\n",
    "# Drop rows with NaN from both y and X_prehosp_numeric\n",
    "y = y.drop(index=nan_and_nd_indexes)\n",
    "X_prehosp_numeric = X_prehosp_numeric.drop(index=nan_and_nd_indexes)\n",
    "\n",
    "# Check if the number of rows matches\n",
    "assert X_prehosp_numeric.shape[0] == y.shape[0], \"Number of rows in X and y do not match!\"\n",
    "\n",
    "print(f\"Number of rows after cleaning: {X_prehosp_numeric.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.to_numpy().ravel()  # Convert y to a 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y), y.shape)  # Type should be numpy.ndarray and shape should be (n_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "N_REPEATS = 3\n",
    "nb_total_samples = len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_smote_under = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf = GridSearchCV(estimator=pipeline_smote_under, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv)\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling = cross_validate(clf, X_prehosp_numeric, y, scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, \n",
    "                                                   cv=outer_cv, n_jobs=-1, return_estimator=True, return_indices=True)\n",
    "\n",
    "print(\"segmentation volumes: HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prehosp_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_scores_smote_undersampling[\"indices\"][\"train\"][fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_train_indices = X_prehosp_numeric.index.intersection(nested_scores_smote_undersampling[\"indices\"][\"train\"][fold])\n",
    "\n",
    "X_prehosp_numeric.loc[good_train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_train = np.asarray(y)[good_train_indices]\n",
    "y_true_test = np.asarray(y)[nested_scores_smote_undersampling[\"indices\"][\"test\"][fold]]\n",
    "\n",
    "fitted = nested_scores_smote_undersampling[\"estimator\"][0].fit(X_prehosp_numeric.loc[good_train_indices], y_true_train) #-#=#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "times = []\n",
    "for n in range(100):\n",
    "    for i in nested_scores_smote_undersampling[\"indices\"][\"test\"][fold]:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            new_pred_test = fitted.predict(X_prehosp_numeric.loc[[i]])\n",
    "            end_time = time.time()\n",
    "            times.append(end_time-start_time)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "\n",
    "print(f\"average time for predictions: \")\n",
    "print(f\"{np.mean(times)} seconds\")\n",
    "print(np.std(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIER = pd.read_csv(DATA_DIRECTORY+\"cleaned_data_full.csv\", usecols=range(60,70), skiprows=1)\n",
    "TIER.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create y based on the conditions and propagate NA values\n",
    "y = pd.DataFrame(index=TIER.index)  # Keep the same indexing as TIL\n",
    "\n",
    "# Apply the conditions, setting NA in y if there are any NA values in the relevant TIL columns\n",
    "y[\"y\"] = TIER.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]].apply(\n",
    "    lambda row: 1 if (row.iloc[6] == 1 or row.iloc[7] == 1 or row.iloc[8] == 1 or row.iloc[9] == 1) else 0, axis=1\n",
    ")\n",
    "\n",
    "# Set y to NaN if any NA exists in the relevant columns\n",
    "y.loc[TIER.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]].isnull().any(axis=1), \"y\"] = pd.NA\n",
    "\n",
    "# Verify the first few rows of y\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome event\n",
    "event_count = (y == 1.00).sum()\n",
    "print(f\"outcome events : {event_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align indexes between y and X_prehosp_numeric\n",
    "X_prehosp_numeric, y = X_prehosp_numeric.align(y, join=\"inner\", axis=0)\n",
    "\n",
    "# Identify rows where any NaN exists in y\n",
    "nan_and_nd_indexes = y.loc[y.isna().any(axis=1)].index\n",
    "\n",
    "# Drop rows with NaN from both y and X_prehosp_numeric\n",
    "y = y.drop(index=nan_and_nd_indexes)\n",
    "X_prehosp_numeric = X_prehosp_numeric.drop(index=nan_and_nd_indexes)\n",
    "\n",
    "# Check if the number of rows matches\n",
    "assert X_prehosp_numeric.shape[0] == y.shape[0], \"Number of rows in X and y do not match!\"\n",
    "\n",
    "print(f\"Number of rows after cleaning: {X_prehosp_numeric.shape[0]}\")\n",
    "\n",
    "y = y.to_numpy().ravel()  # Convert y to a 1D array\n",
    "\n",
    "print(type(y), y.shape)  # Type should be numpy.ndarray and shape should be (n_samples,)\n",
    "\n",
    "FOLDS = 5\n",
    "N_REPEATS = 3\n",
    "nb_total_samples = len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_smote_under = Pipeline(steps=[('over', SMOTE()), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "#pipeline_smote_under = Pipeline(steps=[('over', SMOTENC(categorical_features=[\"fracas_du_bassin\", \"amputation\"])), ('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', HistGradientBoostingClassifier())])\n",
    "\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=5, random_state=1)\n",
    "\n",
    "p_grid = {\"model__learning_rate\": [0.01, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1], \"over__sampling_strategy\": [0.1, 0.2, 0.3], \"over__k_neighbors\":[3,5,8], \"under__sampling_strategy\":[0.3, 0.5, 0.7]}\n",
    "clf = GridSearchCV(estimator=pipeline_smote_under, param_grid=p_grid, scoring={'F2':ftwo_scorer}, refit='F2', cv=inner_cv)\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "nested_scores_smote_undersampling = cross_validate(clf, X_prehosp_numeric, y, scoring={'F2':ftwo_scorer, 'ROC_AUC':'roc_auc', 'Recall':'recall_macro', 'F1':'f1', 'Brier':\"neg_brier_score\", 'False_neg_scorer':false_neg_scorer, 'False_pos_scorer':false_pos_scorer}, cv=outer_cv, n_jobs=-1)\n",
    "\n",
    "print(\"segmentation volumes: HistGradientBoostingClassifier with hyperparameter gridsearch\")\n",
    "\n",
    "roc_auc_metric = np.mean(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "roc_auc_metric_std = np.std(nested_scores_smote_undersampling[\"test_ROC_AUC\"])\n",
    "print(f'AUC (max): {np.round(roc_auc_metric, 2)} +- {np.round(roc_auc_metric_std, 2)}')\n",
    "\n",
    "f1_score = np.mean(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "f1_score_std = np.std(nested_scores_smote_undersampling[\"test_F1\"])\n",
    "print(f'F1 Score (max): {np.round(f1_score, 2)} +- {np.round(f1_score_std, 2)}')\n",
    "\n",
    "f2_score = np.mean(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "f2_score_std = np.std(nested_scores_smote_undersampling[\"test_F2\"])\n",
    "print(f'F2 Score (max): {np.round(f2_score, 2)} +- {np.round(f2_score_std, 2)}')\n",
    "\n",
    "brier_score = -np.mean(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "brier_score_std = -np.std(nested_scores_smote_undersampling[\"test_Brier\"])\n",
    "print(f'Brier Score (min): {np.round(brier_score, 2)} +- {np.round(brier_score_std, 2)}')\n",
    "\n",
    "# test_False_neg_scorer returns the number of test false negatives -> to get a % we need to divide by the number of test samples*100\n",
    "false_neg_score = np.mean(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "false_neg_score_std = np.std(nested_scores_smote_undersampling[\"test_False_neg_scorer\"])*100/(nb_total_samples/FOLDS) \n",
    "print(f'False negative: {int(np.round(false_neg_score, 0))}% +- {int(np.round(false_neg_score_std, 0))}')\n",
    "\n",
    "false_pos_score = np.mean(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "false_pos_score_std = np.std(nested_scores_smote_undersampling[\"test_False_pos_scorer\"])*100/(nb_total_samples/FOLDS)\n",
    "print(f'False positive: {int(np.round(false_pos_score, 0))}% +- {int(np.round(false_pos_score_std, 0))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_env",
   "language": "python",
   "name": "general_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
